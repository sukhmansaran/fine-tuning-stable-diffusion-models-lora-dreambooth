{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sukhmansaran/dual-phase-lora-image-generation-sd-models?scriptVersionId=254720593\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"Please read README.md file on my github for more information.\n\nhttps://github.com/sukhmansaran/fine-tuning-stable-diffusion-models-lora-dreambooth","metadata":{}},{"cell_type":"markdown","source":"# DreamBooth + LoRA Inference Pipeline for Stable Diffusion Models","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\nfrom torchvision import transforms\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer\nfrom diffusers import StableDiffusionPipeline, UNet2DConditionModel\nfrom diffusers.models.attention_processor import LoRAAttnProcessor\nfrom accelerate import Accelerator","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# HF TOKEN\nfrom huggingface_hub import login\nlogin(\"your_token\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Downloading Realistic Vision V5\nfrom huggingface_hub import hf_hub_download\nimport os\n\nmodel_dir = \"your_dir_for_saving_downloaded_base_model\"\nos.makedirs(model_dir, exist_ok=True)\n\nckpt_path = hf_hub_download(\n    repo_id=\"SG161222/Realistic_Vision_V5.1_noVAE\",\n    filename=\"Realistic_Vision_V5.1.safetensors\",\n    local_dir=model_dir,\n)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This is inference file necessary. The file has to be this exact file as it was used for stable diffusion 1.5 version you cannot use any other file here.","metadata":{}},{"cell_type":"code","source":"# Downloading v1-inference.yaml\n!wget -O v1-inference.yaml https://raw.githubusercontent.com/CompVis/stable-diffusion/main/configs/stable-diffusion/v1-inference.yaml","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert to diffusers format\nfrom diffusers.pipelines.stable_diffusion.convert_from_ckpt import download_from_original_stable_diffusion_ckpt\nimport os\n\nsafetensors_path = \"downloaded_base_model_path\"\noutput_dir = \"your_dir_for_saving_converted_base_model\"\n\nconverted_pipeline = download_from_original_stable_diffusion_ckpt(\n    safetensors_path,\n    \"/v1-inference.yaml\",  # Must match SD1.5 or SD2.x\n    from_safetensors=True,\n    extract_ema=True,\n    device=\"cuda\"  # or \"cpu\"\n)\n\n# saving\nconverted_pipeline.save_pretrained(output_dir)\nprint(f\"Model saved to {output_dir}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Model page: https://huggingface.co/SG161222/Realistic_Vision_V5.1_noVAE\n\n⚠️ If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/SG161222/Realistic_Vision_V5.1_noVAE)\n\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) 🙏","metadata":{}},{"cell_type":"code","source":"# Paths\nbase_model_path = \"your_base_model_path\"  # your base model dir\noutput_dir = \"./outputs\"\nos.makedirs(output_dir, exist_ok=True)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Creating the pipeline for loading the model and then using it for inference.","metadata":{}},{"cell_type":"code","source":"from diffusers.schedulers.scheduling_dpmsolver_multistep import DPMSolverMultistepScheduler\n\n# Load pipeline\npipe = StableDiffusionPipeline.from_pretrained(\n    base_model_path,\n    torch_dtype=torch.float16,\n    safety_checker=None,\n    requires_safety_checker=False,\n)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.to(\"cuda\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The below code patches and loads trained weights for unet and text_encoder into the Stable Diffusion model used during the fine tuning process. This patches and loads both phase 1 and phase 2 weights. You just have to provide the correct rank and alpha used in both the phases using both. If one thing wrong will result into error and not patching the lora.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom safetensors.torch import load_file\n\n# Dual-phase LoRA wrapper\nclass LoRALinearDualPhase(nn.Module):\n    def __init__(self, linear, rank1, alpha1, rank2, alpha2):\n        super().__init__()\n        self.linear = linear\n\n        self.rank1 = rank1\n        self.alpha1 = alpha1\n        self.rank2 = rank2\n        self.alpha2 = alpha2\n\n        self.scale1 = alpha1 / rank1\n        self.scale2 = alpha2 / rank2\n        self.phase2_weight = 1.0  # default; can be changed during inference\n\n        self.lora_down = nn.Linear(linear.in_features, rank1, bias=False)\n        self.lora_up = nn.Linear(rank1, linear.out_features, bias=False)\n        self.lora2_down = nn.Linear(linear.in_features, rank2, bias=False)\n        self.lora2_up = nn.Linear(rank2, linear.out_features, bias=False)\n\n        nn.init.zeros_(self.lora_up.weight)\n        nn.init.zeros_(self.lora2_up.weight)\n        nn.init.kaiming_uniform_(self.lora_down.weight, a=5**0.5)\n        nn.init.kaiming_uniform_(self.lora2_down.weight, a=5**0.5)\n\n        # Ensure correct device/dtype\n        for layer in [self.lora_down, self.lora_up, self.lora2_down, self.lora2_up]:\n            layer.to(linear.weight.device, dtype=linear.weight.dtype)\n\n    def forward(self, x):\n        out = self.linear(x)\n\n        lora1 = self.lora_up(self.lora_down(x)) * (self.alpha1 / self.rank1)\n        lora2 = self.lora2_up(self.lora2_down(x)) * (self.alpha2 / self.rank2)\n\n        blended = (1 - self.phase2_weight) * lora1 + self.phase2_weight * lora2\n        return out + blended\n\n# Patch UNet cross-attn with dual-phase LoRA\ndef patch_unet_cross_attn_with_dual_lora(unet, rank1, alpha1, rank2, alpha2):\n    for module in unet.modules():\n        if hasattr(module, 'to_q') and hasattr(module, 'to_k') and hasattr(module, 'to_v'):\n            for attr in ['to_q', 'to_k', 'to_v']:\n                original = getattr(module, attr)\n                if isinstance(original, nn.Linear) and not isinstance(original, LoRALinearDualPhase):\n                    dual_lora = LoRALinearDualPhase(original, rank1, alpha1, rank2, alpha2)\n                    setattr(module, attr, dual_lora)\n\n            # Handle to_out[0] if it's a linear layer\n            if hasattr(module, 'to_out') and isinstance(module.to_out, nn.ModuleList):\n                if isinstance(module.to_out[0], nn.Linear) and not isinstance(module.to_out[0], LoRALinearDualPhase):\n                    original = module.to_out[0]\n                    dual_lora = LoRALinearDualPhase(original, rank1, alpha1, rank2, alpha2)\n                    module.to_out[0] = dual_lora\n    print(\"Patched UNet with Dual-Phase LoRA\")\n\n# Load dual-phase LoRA weights into model\ndef apply_dual_lora_weights(model, lora_path):\n    print(\"Applying Dual-Phase LoRA weights...\")\n    state_dict = load_file(lora_path, device=\"cuda\")\n    missing = []\n\n    for name, param in model.named_parameters():\n        if \"lora\" in name:\n            if name in state_dict:\n                param.data.copy_(state_dict[name])\n            else:\n                missing.append(name)\n\n    print(\"LoRA weights loaded.\")\n    if missing:\n        print(\"Missing LoRA keys:\", missing)\n\n# Patch CLIP TextEncoder with dual-phase LoRA\ndef patch_text_encoder_with_dual_lora(text_encoder, rank1, alpha1, rank2, alpha2):\n    total_patched = 0\n    for module in text_encoder.modules():\n        if all(hasattr(module, attr) for attr in ['q_proj', 'k_proj', 'v_proj', 'out_proj']):\n            for attr in ['q_proj', 'k_proj', 'v_proj', 'out_proj']:\n                original = getattr(module, attr)\n                if isinstance(original, nn.Linear) and not isinstance(original, LoRALinearDualPhase):\n                    dual_lora = LoRALinearDualPhase(original, rank1, alpha1, rank2, alpha2)\n                    setattr(module, attr, dual_lora)\n                    total_patched += 1\n\n    if total_patched > 0:\n        print(f\"Patched {total_patched} layers in TextEncoder with Dual-Phase LoRA\")\n    else:\n        print(\"No layers patched in TextEncoder\")\n\n# Load dual-phase LoRA weights into TextEncoder\ndef apply_dual_lora_weights_to_text_encoder(text_encoder, lora_state_dict):\n    missing = []\n    for name, param in text_encoder.named_parameters():\n        if \"lora\" in name:\n            if name in lora_state_dict:\n                param.data.copy_(lora_state_dict[name])\n            else:\n                missing.append(name)\n    print(\"TextEncoder LoRA weights loaded.\")\n    if missing:\n        print(\"Missing text encoder LoRA keys:\", missing)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This code below patches for both phase 1 and phase 2 weights that can be loaded into our pipeline using the ranks and alphas used during fine tuning. \n\n**Note:** The phase 1 rank and alpha along with phase 2 rank and alpha must be same as used during the fine tuning process.","metadata":{}},{"cell_type":"code","source":"# Patch UNet and Text Encoder\npatch_unet_cross_attn_with_dual_lora(pipe.unet, rank1=4, alpha1=8, rank2=4, alpha2=8)\npatch_text_encoder_with_dual_lora(pipe.text_encoder, rank1=4, alpha1=8, rank2=4, alpha2=8)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This code loads our phase 1 and phase 2 weights into the pipeline.","metadata":{}},{"cell_type":"code","source":"from safetensors.torch import load_file\n\nstate_dict = load_file(\"your_lora_weights_path\", device=\"cuda\")\n\napply_dual_lora_weights(pipe.unet, \"your_lora_weights_path\")\napply_dual_lora_weights_to_text_encoder(pipe.text_encoder, state_dict)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This code helps set weight for phase 2 during the inference. By default both the weights are set to equal weightage during inference.","metadata":{}},{"cell_type":"code","source":"def set_phase_weight(model, phase2_weight: float):\n    \"\"\"\n    Set the blending factor between Phase 1 and Phase 2 LoRA weights.\n    Args:\n        model: The model (e.g., unet or text_encoder) patched with LoRALinearDualPhase.\n        phase2_weight (float): Value between 0.0 (Phase 1 only) and 1.0 (Phase 2 only).\n    \"\"\"\n    for module in model.modules():\n        if isinstance(module, LoRALinearDualPhase):\n            module.phase2_weight = phase2_weight\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The inference process make sure to use your trigger_word here. You can change your prompt, negative prompt, guidance scale, num_inference_steps, height and width. \n\nGuidance scale recommended is 5.5-7.5\n\nNum inference steps recommeded are 30-50\n\nYou must use standard images sizes don't use custom sizes as you like as the model may give unnatural, messy results.","metadata":{}},{"cell_type":"code","source":"# Define inference settings\nprompt = f\"{trigger_word}, your prompt\"\nnegative_prompt = (\n    \"blurry, low resolution, grainy, overexposed, underexposed, bad lighting, jpeg artifacts, glitch, \"\n    \"cropped, out of frame, watermark, duplicate, poorly drawn face, asymmetrical face, deformed features, \"\n    \"bad skin texture, doll-like face, bad eyes, mutated hands, extra fingers, unrealistic proportions, \"\n    \"cartoon, anime, illustration, painting, horror, morbid\"\n)\nguidance_scale = 6.5\nnum_inference_steps = 30\nheight = 768\nwidth = 768\n\n# set phase 2 weightage during inference here (e.g. phase2_weightage=0.3 leads to phase1_weightage=0.7)\nset_phase_weight(pipe.unet, phase2_weight=0.3)\nset_phase_weight(pipe.text_encoder, phase2_weight=0.3)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This code generates and saves your images.","metadata":{}},{"cell_type":"code","source":"# Generate and save images\nimage = pipe(\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    height=height,\n    width=width,\n    num_inference_steps=num_inference_steps,\n    guidance_scale=guidance_scale\n).images[0]\n\nimage.save(os.path.join(output_dir, \"output.png\"))\n# image.show()\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}