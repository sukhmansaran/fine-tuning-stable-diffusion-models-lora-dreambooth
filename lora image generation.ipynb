{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"},"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sukhmansaran/lora-image-generation-sd-models?scriptVersionId=254644590\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"Please read README.md file on my github for more information.\n\nhttps://github.com/sukhmansaran/fine-tuning-stable-diffusion-models-lora-dreambooth","metadata":{}},{"cell_type":"markdown","source":"# DreamBooth + LoRA Inference Pipeline for Stable Diffusion Models","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\nfrom torchvision import transforms\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer\nfrom diffusers import StableDiffusionPipeline, UNet2DConditionModel\nfrom diffusers.models.attention_processor import LoRAAttnProcessor\nfrom accelerate import Accelerator","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# HF TOKEN\nfrom huggingface_hub import login\nlogin(\"your_token\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Downloading Realistic Vision V5\nfrom huggingface_hub import hf_hub_download\nimport os\n\nmodel_dir = \"your_dir_for_saving_downloaded_base_model\"\nos.makedirs(model_dir, exist_ok=True)\n\nckpt_path = hf_hub_download(\n    repo_id=\"SG161222/Realistic_Vision_V5.1_noVAE\",\n    filename=\"Realistic_Vision_V5.1.safetensors\",\n    local_dir=model_dir,\n)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This is inference file necessary. The file has to be this exact file as it was used for stable diffusion 1.5 version you cannot use any other file here.","metadata":{}},{"cell_type":"code","source":"# @title Downloading v1-inference.yaml\n!wget -O v1-inference.yaml https://raw.githubusercontent.com/CompVis/stable-diffusion/main/configs/stable-diffusion/v1-inference.yaml","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert to diffusers format\nfrom diffusers.pipelines.stable_diffusion.convert_from_ckpt import download_from_original_stable_diffusion_ckpt\nimport os\n\nsafetensors_path = \"downloaded_base_model_paths\"\noutput_dir = \"your_dir_for_saving_converted_base_model\"\n\nconverted_pipeline = download_from_original_stable_diffusion_ckpt(\n    safetensors_path,\n    \"/v1-inference.yaml\",  # Must match SD1.5 or SD2.x\n    from_safetensors=True,\n    extract_ema=True,\n    device=\"cuda\"  # or \"cpu\"\n)\n\n# saving\nconverted_pipeline.save_pretrained(output_dir)\nprint(f\"Model saved to {output_dir}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Model page: https://huggingface.co/SG161222/Realistic_Vision_V5.1_noVAE\n\n‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/SG161222/Realistic_Vision_V5.1_noVAE)\n\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè","metadata":{}},{"cell_type":"code","source":"# ‚úÖ Paths\nbase_model_path = \"your_base_model_path\"  # your base model dir\nlora_path = \"your_lora_path\"  # folder containing your LORA weights\noutput_dir = \"./outputs\"\nos.makedirs(output_dir, exist_ok=True)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Creating the pipeline for loading the model and then using it for inference.","metadata":{}},{"cell_type":"code","source":"from diffusers.schedulers.scheduling_dpmsolver_multistep import DPMSolverMultistepScheduler\n\n# ‚úÖ Load pipeline\npipe = StableDiffusionPipeline.from_pretrained(\n    base_model_path,\n    torch_dtype=torch.float16,\n    safety_checker=None,\n    requires_safety_checker=False,\n)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.to(\"cuda\")\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The below code patches and loads trained weights for unet and text_encoder into the Stable Diffusion model used during the fine tuning process.","metadata":{}},{"cell_type":"code","source":"# patching to_q, to_k, to_v and to_out\n\nimport torch\nimport torch.nn as nn\nfrom safetensors.torch import load_file\n\n# ‚úÖ LoRA wrapper\nclass LoRALinear(nn.Module):\n    def __init__(self, linear, rank, alpha):\n        super().__init__()\n        self.linear = linear\n        self.rank = rank\n        self.alpha = alpha\n        self.scaling = alpha / rank\n\n        self.lora_down = nn.Linear(linear.in_features, rank, bias=False)\n        self.lora_up = nn.Linear(rank, linear.out_features, bias=False)\n\n        nn.init.zeros_(self.lora_up.weight)\n        nn.init.kaiming_uniform_(self.lora_down.weight, a=5**0.5)\n\n        self.lora_down.to(linear.weight.device, dtype=linear.weight.dtype)\n        self.lora_up.to(linear.weight.device, dtype=linear.weight.dtype)\n\n    def forward(self, x):\n        return self.linear(x) + self.lora_up(self.lora_down(x)) * self.scaling\n\n# ‚úÖ Patch UNet with LoRA ‚Äî q, k, v, and to_out[0]\ndef patch_unet_cross_attn_with_lora(unet, rank, alpha):\n    for module in unet.modules():\n        if hasattr(module, 'to_q') and hasattr(module, 'to_k') and hasattr(module, 'to_v'):\n            for attr in ['to_q', 'to_k', 'to_v']:\n                original = getattr(module, attr)\n                if isinstance(original, nn.Linear) and not isinstance(original, LoRALinear):\n                    lora_layer = LoRALinear(original, rank=rank, alpha=alpha).to(original.weight.device)\n                    setattr(module, attr, lora_layer)\n\n            # Handle to_out[0] if it's a linear layer\n            if hasattr(module, 'to_out') and isinstance(module.to_out, nn.ModuleList):\n                if isinstance(module.to_out[0], nn.Linear) and not isinstance(module.to_out[0], LoRALinear):\n                    original = module.to_out[0]\n                    lora_layer = LoRALinear(original, rank=rank, alpha=alpha).to(original.weight.device)\n                    module.to_out[0] = lora_layer\n\n# ‚úÖ Load LoRA weights\ndef apply_lora_weights(unet, lora_path):\n    print(\"üîÑ Applying LoRA weights...\")\n    state_dict = load_file(lora_path, device=\"cuda\")\n    missing = []\n\n    for name, param in unet.named_parameters():\n        if \"lora\" in name:\n            if name in state_dict:\n                param.data.copy_(state_dict[name])\n            else:\n                missing.append(name)\n\n    print(\"‚úÖ LoRA weights loaded.\")\n    if missing:\n        print(\"‚ö†Ô∏è Missing LoRA keys:\", missing)\n\n# ‚úÖ Patch CLIP TextEncoder Attention (q_proj, k_proj, v_proj, out_proj)\ndef patch_text_encoder_attention_with_lora(text_encoder, rank, alpha):\n    for module in text_encoder.modules():\n        if all(hasattr(module, attr) for attr in ['q_proj', 'k_proj', 'v_proj', 'out_proj']):\n            for attr in ['q_proj', 'k_proj', 'v_proj', 'out_proj']:\n                original = getattr(module, attr)\n                if isinstance(original, nn.Linear) and not isinstance(original, LoRALinear):\n                    lora_layer = LoRALinear(original, rank=rank, alpha=alpha).to(original.weight.device)\n                    setattr(module, attr, lora_layer)\n\n# ‚úÖ Load LoRA weights into Text Encoder\ndef apply_lora_weights_to_text_encoder(text_encoder, lora_state_dict):\n    missing = []\n    for name, param in text_encoder.named_parameters():\n        if \"lora\" in name:\n            if name in lora_state_dict:\n                param.data.copy_(lora_state_dict[name])\n            else:\n                missing.append(name)\n    print(\"‚úÖ Text encoder LoRA weights loaded.\")\n    if missing:\n        print(\"‚ö†Ô∏è Missing text encoder LoRA keys:\", missing)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This function code patches and loads our unet LORA weights into our pipeline. You have to use the same rank and alpha used during the fine tuning process if they mismatch it will cause error and not patch the layers.","metadata":{}},{"cell_type":"code","source":"# ‚úÖ Apply to loaded pipeline UNet\npatch_unet_cross_attn_with_lora(pipe.unet, rank=4, alpha=8)\napply_lora_weights(pipe.unet, f\"{lora_path}/diffusion_pytorch_model.safetensors\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Same as unet this function code patches and loads our text_encoder LORA weights into our pipeline. You have to use the same rank and alpha used during the fine tuning process if they mismatch it will cause error and not patch the layers.","metadata":{}},{"cell_type":"code","source":"# ‚úÖ Apply to loaded pipeline Text Encoder\npatch_text_encoder_attention_with_lora(pipe.text_encoder, rank=4, alpha=8)\n\n# Reuse same weights already loaded (avoid re-loading from disk)\nlora_state_dict = load_file(f\"{lora_path}/diffusion_pytorch_model.safetensors\", device=\"cuda\")\napply_lora_weights_to_text_encoder(pipe.text_encoder, lora_state_dict)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The inference process make sure to use your trigger_word here. You can change your prompt, negative prompt, guidance scale, num_inference_steps, height and width. \n\nGuidance scale recommended is 5.5-7.5\n\nNum inference steps recommeded are 30-50\n\nYou must use standard images sizes don't use custom sizes as you like as the model may give unnatural, messy results.","metadata":{}},{"cell_type":"code","source":"# ‚úÖ Define inference settings\nprompt = f\"{trigger_word}, your prompt\"\nnegative_prompt = (\n    \"blurry, low resolution, grainy, overexposed, underexposed, bad lighting, jpeg artifacts, glitch, \"\n    \"cropped, out of frame, watermark, duplicate, poorly drawn face, asymmetrical face, deformed features, \"\n    \"bad skin texture, doll-like face, bad eyes, mutated hands, extra fingers, unrealistic proportions, \"\n    \"cartoon, anime, illustration, painting, horror, morbid\"\n)\nguidance_scale = 6.5\nnum_inference_steps = 30\nheight = 768\nwidth = 768\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This code generates and saves your images.","metadata":{}},{"cell_type":"code","source":"# ‚úÖ Generate and save images\nimage = pipe(\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    height=height,\n    width=width,\n    num_inference_steps=num_inference_steps,\n    guidance_scale=guidance_scale\n).images[0]\n\nimage.save(os.path.join(output_dir, \"output.png\"))\n# image.show()\n","metadata":{},"outputs":[],"execution_count":null}]}