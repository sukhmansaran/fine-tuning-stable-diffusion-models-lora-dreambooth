{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DreamBooth + LoRA Training Pipeline for Stable Diffusion Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for fine tune for new styles, new designs, new chacracters. The addition in this file is that you can also train feedforward layers now. These layers are necessary for making the model learn complex images patterns and transformations. Fine-tuning feedforward layers is typically not necessary for learning new styles or characters, but you can always make them patch and trainable. This will lead to more resource usage and more parameters being trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from diffusers import StableDiffusionPipeline, UNet2DConditionModel\n",
    "from diffusers.models.attention_processor import LoRAAttnProcessor\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HF TOKEN\n",
    "from huggingface_hub import login\n",
    "login(\"your_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading Realistic Vision V5\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "\n",
    "model_dir = \"your_dir_for_saving_downloaded_base_model\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "ckpt_path = hf_hub_download(\n",
    "    repo_id=\"SG161222/Realistic_Vision_V5.1_noVAE\",\n",
    "    filename=\"Realistic_Vision_V5.1.safetensors\",\n",
    "    local_dir=model_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is inference file necessary. The file has to be this exact file as it was used for stable diffusion 1.5 version you cannot use any other file here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Downloading v1-inference.yaml\n",
    "!wget -O v1-inference.yaml https://raw.githubusercontent.com/CompVis/stable-diffusion/main/configs/stable-diffusion/v1-inference.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to diffusers format\n",
    "from diffusers.pipelines.stable_diffusion.convert_from_ckpt import download_from_original_stable_diffusion_ckpt\n",
    "\n",
    "safetensors_path = \"downloaded_base_model_path\"\n",
    "output_dir = \"your_dir_for_saving_converted_base_model\"\n",
    "\n",
    "converted_pipeline = download_from_original_stable_diffusion_ckpt(\n",
    "    safetensors_path,\n",
    "    \"/v1-inference.yaml\",  # Must match SD1.5 or SD2.x\n",
    "    from_safetensors=True,\n",
    "    extract_ema=True,\n",
    "    device=\"cuda\"  # or \"cpu\"\n",
    ")\n",
    "\n",
    "# saving\n",
    "converted_pipeline.save_pretrained(output_dir)\n",
    "print(f\"Model saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining configurations for the training**\n",
    "\n",
    "trigger_word: This let's you call your character. Whenever you use this it means that you are calling/referring your character.\n",
    "\n",
    "resolution: the resolution for training your images (Recommeded is 512x512 as it is what base model was trained on. You can you 768x768 or 1024x1024, these resolutions will lead to more resource usage and unstable fine tuning.)\n",
    "\n",
    "batch_size: how many images it will train in each step\n",
    "\n",
    "gradient_accumulation: Using this I can increase my batch size with same GPU memory usage.\n",
    "\n",
    "effective batch size = (batch_size) x (gradient_accumulation)\n",
    "\n",
    "learning_rate: how fast you want the model to learn the character (too high the model might overfit, too low the model would not learn your character fully) (effective proven and tested learning rate would be from 5e-5 to 1e-6 and depending upon the training steps and dataset variation) \n",
    "\n",
    "max_train_steps: how many steps you want the training to run (1 step = 1 batch run)\n",
    "\n",
    "train_text_encoder: whether we want to train train_text_encoder or not. (if yes we would update weights of train text encoder used in the base model using our setup) (Note: training this is necessary when training on specific characters as the base model might struggle learning your trigger_word and how it is in different scenarios)\n",
    "\n",
    "lr_scheduler: how do you want your learning rate to decay. (I tried constant, no lr scheduler it doesn't work, it model overfitted or underfitted, cosine worked best for me as the learning_rate would decay slowly.)\n",
    "\n",
    "lr_warmup_steps: when you want your learning rate to start decay (e.g. 100 means leraning rate would start decaying after 100 steps with the lr_scheduler)\n",
    "\n",
    "lora_r: Determines the rank of the low-rank adaptation matrices, controlling the number of trainable parameters added for fine-tuning.\n",
    "\n",
    "lora_alpha: A scaling factor applied to the LoRA updates, adjusting their overall impact on the model's weights.\n",
    "\n",
    "lora_dropout: Probability of randomly dropping LoRA adaptation connections during training, which helps prevent overfitting by adding regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # ✅ Paths\n",
    "    model_path = \"your_downloaded_model_path\"\n",
    "    dataset_dir = \"your_dataset_path\"\n",
    "    output_dir = \"your_output_dir\"\n",
    "\n",
    "    # ✅ Training Behavior\n",
    "    trigger_word = \"sks\"  # Keep this for identity consistency\n",
    "    resolution = 512       # Good middle ground for face detail\n",
    "    \n",
    "    batch_size = 1\n",
    "    gradient_accumulation = 4\n",
    "    learning_rate = 4e-5   # \n",
    "    max_train_steps = 3330 # \n",
    "    \n",
    "    mixed_precision = \"fp16\"\n",
    "    train_text_encoder = True  \n",
    "\n",
    "    # ✅ LoRA Settings\n",
    "    lr_scheduler = \"cosine\"\n",
    "    lr_warmup_steps = 185\n",
    "    lora_r = 4\n",
    "    lora_alpha = 8\n",
    "    lora_dropout = 0.1\n",
    "    # lora_target_modules = [\"CrossAttention\", \"Attention\"]\n",
    "\n",
    "    # ✅ Logging & Checkpoints\n",
    "    save_every_n_steps = 1110\n",
    "    log_every_n_steps = 370\n",
    "    generate_every_n_steps = 370\n",
    "    seed = 151101\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, inspect\n",
    "\n",
    "def save_config(cfg, path=None):\n",
    "    if path is None:\n",
    "        path = os.path.join(cfg.output_dir, \"lora_config.json\")\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "    # grab only data attributes declared on the *class*\n",
    "    config_dict = {\n",
    "        k: v for k, v in cfg.__class__.__dict__.items()\n",
    "        if not k.startswith(\"__\") and not inspect.isfunction(v) and not inspect.ismethod(v)\n",
    "    }\n",
    "\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(config_dict, f, indent=4)\n",
    "    print(f\"✅  Config saved to {path}\")\n",
    "\n",
    "save_config(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "torch.manual_seed(cfg.seed)\n",
    "torch.cuda.manual_seed(cfg.seed)\n",
    "random.seed(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Loader\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, image_dir, tokenizer, size):\n",
    "        self.image_paths = []\n",
    "        self.caption_paths = []\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        for fname in sorted(os.listdir(image_dir)):\n",
    "            if fname.endswith(\".png\") or fname.endswith(\".jpg\"):\n",
    "                img_path = os.path.join(image_dir, fname)\n",
    "                txt_path = os.path.splitext(img_path)[0] + \".txt\"\n",
    "                if os.path.exists(txt_path):\n",
    "                    self.image_paths.append(img_path)\n",
    "                    self.caption_paths.append(txt_path)\n",
    "\n",
    "        self.image_transforms = transforms.Compose([\n",
    "            transforms.Resize((size, size), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        if image.getbbox() is None:\n",
    "            raise ValueError(f\"Empty image found: {self.image_paths[idx]}\")\n",
    "        else:\n",
    "            image = self.image_transforms(image)\n",
    "            with open(self.caption_paths[idx], \"r\") as f:\n",
    "                caption = f.read().strip()\n",
    "\n",
    "            inputs = self.tokenizer(caption, truncation=True, padding=\"max_length\", max_length=77, return_tensors=\"pt\")\n",
    "            \n",
    "        return {\"pixel_values\": image, \"input_ids\": inputs.input_ids.squeeze(0)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was frustrated by version mismatches and compatibility issues with popular libraries (like peft) for LoRA in pytorch. Instead of fighting package dependencies, I researched the fundamental principles behind LoRA and wrote a minimal, robust implementation compatible with any pytorch model using nn.Linear layers.\n",
    "\n",
    "My LoRALinear class is a simple, direct wrapper for any nn.Linear layer. It adds trainable “LoRA” weights on top of the original weights, enabling efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# 🔧 Safer and more flexible LoRA wrapper for nn.Linear\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, linear: nn.Linear, rank: int, alpha: float):\n",
    "        super().__init__()\n",
    "\n",
    "        if not isinstance(linear, nn.Linear):\n",
    "            raise TypeError(f\"LoRALinear can only wrap nn.Linear, but got {type(linear)}\")\n",
    "\n",
    "        self.linear = linear\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "\n",
    "        # LoRA layers\n",
    "        self.lora_down = nn.Linear(linear.in_features, rank, bias=False)\n",
    "        self.lora_up = nn.Linear(rank, linear.out_features, bias=False)\n",
    "\n",
    "        # Initialization (standard LoRA practice)\n",
    "        nn.init.kaiming_uniform_(self.lora_down.weight, a=5**0.5)\n",
    "        nn.init.zeros_(self.lora_up.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Original + LoRA residual\n",
    "        return self.linear(x) + self.lora_up(self.lora_down(x)) * self.scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our patcher for patching unet.\n",
    "\n",
    "The most important question what are we patching, why that part and we would we patch?\n",
    "\n",
    "Answer: We would patch some layers of the model not all, the layers most important for learning, new style, new character, new things are \"to_q\" ,\"to_k\", \"to_v\" and \"to_out\". These layers work together when you train your model they contain their own weights for what they learned. If we patched the old weights we might mess with the working of the model and it may produce messy, or noisy results during image generation. That's why we patch new layers and train the model these contain new weights for your specific style, character and new things. And you can load these on top of your Stable Diffusion Model and use them for image generation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_unet_cross_attn_with_lora(unet, rank, alpha):\n",
    "    lora_params = []\n",
    "\n",
    "    for module in unet.modules():\n",
    "        # ── Patch cross-attention layers (q, k, v, out) ──\n",
    "        for attr in ['to_q', 'to_k', 'to_v', 'to_out']:\n",
    "            if hasattr(module, attr):\n",
    "                original = getattr(module, attr)\n",
    "\n",
    "                # ✅ Patch Linear directly\n",
    "                if isinstance(original, nn.Linear) and not isinstance(original, LoRALinear):\n",
    "                    lora_layer = LoRALinear(original, rank=rank, alpha=alpha)\n",
    "                    setattr(module, attr, lora_layer)\n",
    "                    lora_params.extend(lora_layer.lora_down.parameters())\n",
    "                    lora_params.extend(lora_layer.lora_up.parameters())\n",
    "                    print(f\"✅ Patched {attr} in {module.__class__.__name__}\")\n",
    "\n",
    "                # 🔍 Special case: ModuleList (e.g. to_out)\n",
    "                elif isinstance(original, nn.ModuleList):\n",
    "                    for i, sublayer in enumerate(original):\n",
    "                        if isinstance(sublayer, nn.Linear) and not isinstance(sublayer, LoRALinear):\n",
    "                            lora_layer = LoRALinear(sublayer, rank=rank, alpha=alpha)\n",
    "                            original[i] = lora_layer\n",
    "                            lora_params.extend(lora_layer.lora_down.parameters())\n",
    "                            lora_params.extend(lora_layer.lora_up.parameters())\n",
    "                            print(f\"✅ Patched {attr}[{i}] in {module.__class__.__name__}\")\n",
    "                        else:\n",
    "                            print(f\"⚠️ Skipping {attr}[{i}] — Not a Linear: {type(sublayer)}\")\n",
    "\n",
    "                else:\n",
    "                    print(f\"⚠️ Skipping {attr} in {module.__class__.__name__} — Not a Linear or ModuleList\")\n",
    "\n",
    "    total_params = sum(p.numel() for p in lora_params)\n",
    "    print(f\"\\n✅ UNet LoRA patched. Total trainable LoRA params: {total_params}\")\n",
    "    return lora_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our patcher for patching feedforward layers of unet in Stable Diffusion models. These layers are \"proj\" layers of Linear modules. Additionaly we would also patch output layers of feedforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_unet_mlp_with_lora(unet, rank, alpha):\n",
    "    lora_params = []\n",
    "\n",
    "    for name, module in unet.named_modules():\n",
    "        # Patch GEGLU.proj (MLP expansion)\n",
    "        if hasattr(module, \"proj\") and isinstance(module.proj, nn.Linear):\n",
    "            if not isinstance(module.proj, LoRALinear):\n",
    "                lora_layer = LoRALinear(module.proj, rank=rank, alpha=alpha)\n",
    "                module.proj = lora_layer\n",
    "                lora_params += list(lora_layer.parameters())\n",
    "                print(f\"✅ Patched {name}.proj\")\n",
    "\n",
    "        # Patch MLP output Linear layers in ModuleList.2\n",
    "        if isinstance(module, nn.ModuleList):\n",
    "            for idx, submodule in enumerate(module):\n",
    "                if idx == 2 and isinstance(submodule, nn.Linear) and not isinstance(submodule, LoRALinear):\n",
    "                    lora_layer = LoRALinear(submodule, rank=rank, alpha=alpha)\n",
    "                    module[idx] = lora_layer\n",
    "                    lora_params += list(lora_layer.parameters())\n",
    "                    print(f\"✅ Patched {name}[2]\")\n",
    "\n",
    "    print(f\"\\n✅ Total LoRA parameters in MLPs: {sum(p.numel() for p in lora_params):,}\")\n",
    "    return lora_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is to unfrreze train_text encoder. I defined it for testing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def unfreeze_text_encoder_attention(text_encoder):\n",
    "#     trainable_params = []\n",
    "#     total_unfroze = 0\n",
    "\n",
    "#     for module in text_encoder.modules():\n",
    "#         if all(hasattr(module, attr) for attr in ['q_proj', 'k_proj', 'v_proj', 'out_proj']):\n",
    "#             for attr in ['q_proj', 'k_proj', 'v_proj', 'out_proj']:\n",
    "#                 proj = getattr(module, attr)\n",
    "#                 for param in proj.parameters():\n",
    "#                     param.requires_grad = True\n",
    "#                     trainable_params.append(param)\n",
    "#                 total_unfroze += 1\n",
    "#                 print(f\"✅ Unfroze text encoder layer: {module.__class__.__name__} → {attr}\")\n",
    "\n",
    "#     print(f\"\\n✅ Total unfrozen attention projection blocks in text encoder: {total_unfroze}\")\n",
    "#     return trainable_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will patch the text_encoder the theory and reason is same as patching the unet layers. We would patch \"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\" of the text_encoder. Additional layers are \"fc1\" and \"fc2\" these are feedforward layers of text encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_text_encoder(text_encoder, rank, alpha):\n",
    "    lora_params = []\n",
    "    target_names = {\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"fc1\", \"fc2\"}\n",
    "\n",
    "    for module in text_encoder.modules():\n",
    "        for name in target_names:\n",
    "            if hasattr(module, name):\n",
    "                layer = getattr(module, name)\n",
    "\n",
    "                # Only patch raw nn.Linear layers (skip if already patched)\n",
    "                if isinstance(layer, nn.Linear) and not isinstance(layer, LoRALinear):\n",
    "                    lora_layer = LoRALinear(layer, rank=rank, alpha=alpha)\n",
    "                    setattr(module, name, lora_layer)\n",
    "\n",
    "                    lora_params += list(lora_layer.lora_down.parameters())\n",
    "                    lora_params += list(lora_layer.lora_up.parameters())\n",
    "\n",
    "                    print(f\"🔧 Patched {module.__class__.__name__}.{name} with LoRA\")\n",
    "\n",
    "    print(f\"✅ Finished patching text encoder — LoRA params: {sum(p.numel() for p in lora_params):,}\")\n",
    "    return lora_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this is where we would create our pipeline, load our model into pipeline, load our tokenizer, load our trigger word into the tokenizer, load our unet and patch it, load our text_encoder and patch it. But the important thing is that we need to freeze our params of unet and text_encoder before patching so we accidently don't train them or update them and we only train or update our newly patched layers. We are using AdamW optimer here originally used with base our base model. We would also load our dataset into the pipeline and prepare everything for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os\n",
    "from accelerate import Accelerator\n",
    "from diffusers          import StableDiffusionPipeline, UNet2DConditionModel\n",
    "from transformers        import AutoTokenizer, get_scheduler\n",
    "from torch.utils.data    import DataLoader\n",
    "\n",
    "# ──────────────────────────────────────────────────────────\n",
    "#  1.  Accelerate & Config\n",
    "# ──────────────────────────────────────────────────────────\n",
    "accelerator = Accelerator(split_batches=True)\n",
    "device       = accelerator.device\n",
    "cfg          = Config()                       # <- your existing config class\n",
    "\n",
    "# ──────────────────────────────────────────────────────────\n",
    "#  2.  Load full pipeline   (VAE, Text‑Encoder, UNet, etc.)\n",
    "# ──────────────────────────────────────────────────────────\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    cfg.model_path,\n",
    "    torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────\n",
    "#  3.  Trigger‑token → add BEFORE any dataset tokenisation\n",
    "# ──────────────────────────────────────────────────────────\n",
    "tokenizer      = AutoTokenizer.from_pretrained(cfg.model_path, subfolder=\"tokenizer\")\n",
    "trigger_token  = cfg.trigger_word                    # e.g. \"pendugwen\"\n",
    "\n",
    "if len(tokenizer.tokenize(trigger_token)) > 1:       # splits → need custom token\n",
    "    tokenizer.add_tokens([trigger_token])\n",
    "    pipe.text_encoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # initialise the new embedding from \"person\"\n",
    "    with torch.no_grad():\n",
    "        emb           = pipe.text_encoder.get_input_embeddings()\n",
    "        new_id        = tokenizer.convert_tokens_to_ids(trigger_token)\n",
    "        base_id       = tokenizer.convert_tokens_to_ids(\"person\")\n",
    "        emb.weight[new_id] = emb.weight[base_id].clone()\n",
    "\n",
    "    print(f\"✅ Added custom token '{trigger_token}' (id {new_id})\")\n",
    "else:\n",
    "    print(f\"✅ '{trigger_token}' already a single token\")\n",
    "\n",
    "pipe.tokenizer = tokenizer        # keep pipeline & dataset in sync\n",
    "\n",
    "# ──────────────────────────────────────────────────────────\n",
    "#  4.  Load UNet separately (so we can LoRA‑patch it)\n",
    "# ──────────────────────────────────────────────────────────\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    cfg.model_path,\n",
    "    subfolder=\"unet\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# Freeze everything first\n",
    "for p in unet.parameters():            p.requires_grad = False\n",
    "for p in pipe.text_encoder.parameters(): p.requires_grad = False\n",
    "\n",
    "# ──────────────────────────────────────────────────────────\n",
    "#  5.  LoRA‑patch UNet  (+ optional text‑encoder)\n",
    "# ──────────────────────────────────────────────────────────\n",
    "lora_params  = patch_unet_cross_attn_with_lora(unet, cfg.lora_r, cfg.lora_alpha)\n",
    "\n",
    "lora_params += patch_unet_mlp_with_lora(unet, cfg.lora_r, cfg.lora_alpha)\n",
    "\n",
    "if cfg.train_text_encoder:\n",
    "    lora_params += patch_text_encoder(pipe.text_encoder, cfg.lora_r, cfg.lora_alpha)\n",
    "    print(\"✅ Text‑encoder LoRA patched\")\n",
    "\n",
    "if not lora_params:\n",
    "    raise RuntimeError(\"No trainable LoRA params collected!\")\n",
    "\n",
    "print(f\"🔍 LoRA trainable parameters: {sum(p.numel() for p in lora_params):,}\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────\n",
    "#  6.  Optimiser & Scheduler (LoRA params only)\n",
    "# ──────────────────────────────────────────────────────────\n",
    "optimizer     = torch.optim.AdamW(lora_params, lr=cfg.learning_rate)\n",
    "\n",
    "lr_scheduler  = get_scheduler(\n",
    "    cfg.lr_scheduler,\n",
    "    optimizer          = optimizer,\n",
    "    num_warmup_steps   = cfg.lr_warmup_steps,\n",
    "    num_training_steps = cfg.max_train_steps,\n",
    ")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────\n",
    "#  7.  Dataset & Dataloader  (tokenizer now has fixed token!)\n",
    "# ──────────────────────────────────────────────────────────\n",
    "dataset    = ImageCaptionDataset(cfg.dataset_dir, tokenizer, cfg.resolution)\n",
    "dataloader = DataLoader(dataset, batch_size=cfg.batch_size, shuffle=True)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────\n",
    "#  8.  Prepare for Accelerate & training\n",
    "# ──────────────────────────────────────────────────────────\n",
    "pipe.text_encoder.to(device, dtype=torch.float16)\n",
    "unet.train()\n",
    "\n",
    "unet, optimizer, dataloader = accelerator.prepare(unet, optimizer, dataloader)\n",
    "\n",
    "print(\"🚀 Setup complete – ready to train LoRA adapters.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check that how many params we are going to train now out of total params. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in unet.parameters()) + sum(p.numel() for p in pipe.text_encoder.parameters())\n",
    "trainable_params = sum(p.numel() for p in unet.parameters() if p.requires_grad) + \\\n",
    "                   sum(p.numel() for p in pipe.text_encoder.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"✅ Trainable parameters: {trainable_params} / {total_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the vae used in the base model. The vae is what encodes images into latent spacs and decodes our latent spaces back into images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKL\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\", torch_dtype=torch.float32)\n",
    "pipe.vae = vae\n",
    "\n",
    "pipe.vae.to(accelerator.device, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accelerator.device)  # Should print: cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check that if our paramters contain NAN values before our training begins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in unet.named_parameters():\n",
    "    if torch.isnan(param).any():\n",
    "        print(f\"NaN detected in UNet parameter: {name}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would create functions for checking if our trigger_word is in the tokenizer, a function to generate sample images during the model training for checks if our model is learning or not, whether it is overfitting or underfitting. Whether if we need to stop our training, whether we need to change our configurations settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from diffusers import DDIMScheduler\n",
    "from torch import autocast\n",
    "\n",
    "# ✅ Optional: Reproducibility\n",
    "def seed_everything(seed=151101):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "# ✅ Optional: Disable NSFW filter (for local testing)\n",
    "def disable_safety(pipe):\n",
    "    pipe.safety_checker = lambda images, **kwargs: (images, [False] * len(images))\n",
    "\n",
    "# ✅ Fix tokenizer trigger word if needed\n",
    "def ensure_trigger_token(pipe, trigger_word):\n",
    "    tokens = pipe.tokenizer.tokenize(trigger_word)\n",
    "    if len(tokens) > 1:\n",
    "        print(f\"⚠️ Trigger word '{trigger_word}' is split: {tokens}. Fixing...\")\n",
    "        pipe.tokenizer.add_tokens([trigger_word])\n",
    "        pipe.text_encoder.resize_token_embeddings(len(pipe.tokenizer))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embeddings = pipe.text_encoder.get_input_embeddings()\n",
    "            init_id = pipe.tokenizer.convert_tokens_to_ids(\"person\")\n",
    "            new_id = pipe.tokenizer.convert_tokens_to_ids(trigger_word)\n",
    "            embeddings.weight[new_id] = embeddings.weight[init_id].clone()\n",
    "\n",
    "        print(f\"✅ Re-added and initialized embedding for trigger word '{trigger_word}'\")\n",
    "    else:\n",
    "        print(f\"✅ Trigger word '{trigger_word}' is tokenized correctly: {tokens}\")\n",
    "\n",
    "# ✅ Generate and save a sample image\n",
    "def generate_sample_image(step, save_path, prompt=None, negative_prompt=None, seed=151101):\n",
    "    print(f\"\\n🎨 Generating preview at step {step}...\")\n",
    "\n",
    "    # ✅ Reproducible randomness\n",
    "    seed_everything(seed)\n",
    "    generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
    "\n",
    "    # ✅ Restore UNet (EMA if available, else current)\n",
    "    if 'ema_unet' in globals() and ema_unet is not None:\n",
    "        pipe.unet = ema_unet\n",
    "        print(\"📦 Using EMA UNet for inference.\")\n",
    "    else:\n",
    "        pipe.unet = accelerator.unwrap_model(unet)\n",
    "        print(\"📦 Using current UNet for inference.\")\n",
    "    \n",
    "    # ✅ Restore LoRA-trained text_encoder if trained\n",
    "    if cfg.train_text_encoder:\n",
    "        pipe.text_encoder = accelerator.unwrap_model(pipe.text_encoder)\n",
    "        print(\"🧠 Restored LoRA-trained text encoder for inference.\")\n",
    "\n",
    "    pipe.unet.eval()\n",
    "    pipe.text_encoder.eval()\n",
    "\n",
    "    # ✅ Disable NSFW checker for previewing\n",
    "    disable_safety(pipe)\n",
    "\n",
    "    # ✅ Ensure tokenizer supports the trigger word\n",
    "    ensure_trigger_token(pipe, cfg.trigger_word)\n",
    "\n",
    "    # ✅ Use fast scheduler\n",
    "    pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "    # ✅ Default prompts if not provided\n",
    "    if prompt is None:\n",
    "        prompt = f\"{cfg.trigger_word}, your prompt\"\n",
    "    if negative_prompt is None:\n",
    "        negative_prompt = (\n",
    "            \"blurry, low resolution, grainy, overexposed, underexposed, poor lighting, jpeg artifacts, glitch, \"\n",
    "            \"cropped, out of frame, watermark, duplicate, poorly drawn face, asymmetrical face, deformed face, \"\n",
    "            \"unnatural skin texture, doll-like face, bad eyes, mutated hands, extra fingers, distorted anatomy, \"\n",
    "            \"unrealistic proportions, cartoon, anime, illustration, painting, horror, morbid\"\n",
    "        )\n",
    "\n",
    "    # ✅ Generate image\n",
    "    with autocast(\"cuda\"):\n",
    "        # Generate 4 images\n",
    "        result = pipe(\n",
    "            prompt=[prompt] * 4,\n",
    "            negative_prompt=[negative_prompt] * 4,\n",
    "            num_inference_steps=30,\n",
    "            guidance_scale=6.0,\n",
    "            height=cfg.resolution,\n",
    "            width=cfg.resolution,\n",
    "            generator=generator,\n",
    "        )\n",
    "\n",
    "    # ✅ Save images\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    for i, image in enumerate(result.images):\n",
    "        save_name = os.path.join(save_path, f\"preview_step_{step}_{i+1}.png\")\n",
    "        image.save(save_name)\n",
    "        print(f\"✅ Saved: {save_name}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our main training loop for training our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.amp import autocast\n",
    "from safetensors.torch import save_file\n",
    "from diffusers.models.attention_processor import LoRAAttnProcessor\n",
    "from PIL import Image\n",
    "\n",
    "# 🛠️ Optional: Force LoRA layers to float32 for stability\n",
    "for module in unet.modules():\n",
    "    if isinstance(module, LoRAAttnProcessor):\n",
    "        for param in module.parameters():\n",
    "            param.data = param.data.to(torch.float32)\n",
    "\n",
    "global_step = 0\n",
    "unet.train()\n",
    "\n",
    "for epoch in range(100):\n",
    "    for step, batch in enumerate(tqdm(dataloader)):\n",
    "        with accelerator.accumulate(unet):\n",
    "            # ✅ 1. Move batch to device\n",
    "            pixel_values = batch[\"pixel_values\"].to(accelerator.device, dtype=torch.float32)  # VAE prefers float32\n",
    "            input_ids = batch[\"input_ids\"].to(accelerator.device)\n",
    "            # print(tokenizer.decode(input_ids[0]))\n",
    "# \n",
    "            # ✅ 2. Encode with VAE (float32), then clamp and convert\n",
    "            with torch.no_grad():\n",
    "                latents = pipe.vae.encode(pixel_values).latent_dist.sample()\n",
    "                latents = latents.clamp(-10, 10)  # Avoid extreme latent values\n",
    "                latents = latents * 0.18215\n",
    "                latents = latents.to(accelerator.device, dtype=torch.float16)\n",
    "\n",
    "            # ✅ 3. Add scaled noise\n",
    "            noise = 0.9 * torch.randn_like(latents)  # reduce intensity for early stability\n",
    "            max_timestep = 300 if global_step < 100 else pipe.scheduler.config.num_train_timesteps\n",
    "            timesteps = torch.randint(0, max_timestep, (latents.shape[0],), device=latents.device).long()\n",
    "            noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "            for name, param in unet.named_parameters():\n",
    "                if torch.isnan(param).any() or torch.isinf(param).any():\n",
    "                    print(f\"[❌ NaN/Inf detected] in parameter: {name}\")\n",
    "                    break\n",
    "\n",
    "\n",
    "            # ✅ 4. Encode text\n",
    "            with torch.no_grad():\n",
    "                encoder_hidden_states = pipe.text_encoder(input_ids)[0]\n",
    "                encoder_hidden_states = encoder_hidden_states.to(accelerator.device, dtype=torch.float16)\n",
    "\n",
    "            # ✅ 5. UNet forward with autocast\n",
    "            with autocast(\"cuda\", dtype=torch.float32):\n",
    "                model_pred = unet(\n",
    "                    noisy_latents,\n",
    "                    timesteps,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                ).sample\n",
    "\n",
    "            # ✅ Debug logs\n",
    "            # print(f\"Latents mean/std: {latents.mean().item():.4f}/{latents.std().item():.4f}\")\n",
    "            # print(f\"model_pred mean/std: {model_pred.mean().item():.4f}/{model_pred.std().item():.4f}\")\n",
    "\n",
    "            # ✅ Check for NaNs\n",
    "            if torch.isnan(model_pred).any():\n",
    "                print(\"❌ NaN in model_pred!\")\n",
    "                print(\"Input stats:\", noisy_latents.mean(), noisy_latents.std())\n",
    "                print(\"Timesteps:\", timesteps)\n",
    "                print(\"Encoder stats:\", encoder_hidden_states.mean(), encoder_hidden_states.std())\n",
    "                continue\n",
    "\n",
    "            # ✅ 6. Compute loss\n",
    "            noise = noise.to(model_pred.dtype)\n",
    "            loss = torch.nn.functional.l1_loss(model_pred, noise)\n",
    "\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                print(f\"⚠️ Skipping invalid loss at step {global_step}\")\n",
    "                continue\n",
    "\n",
    "            # ✅ 7. Backward + optimizer\n",
    "            accelerator.backward(loss)\n",
    "            if accelerator.sync_gradients:\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                # ✅ Logging\n",
    "                if global_step % cfg.log_every_n_steps == 0:\n",
    "                    print(f\"Step {global_step} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "                # ✅ Save LoRA Weights\n",
    "                if global_step % cfg.save_every_n_steps == 0:\n",
    "                    save_path = os.path.join(cfg.output_dir, f\"step_{global_step}\")\n",
    "                    os.makedirs(save_path, exist_ok=True)\n",
    "                \n",
    "                    def extract_lora_weights(state_dict):\n",
    "                        return {k: v for k, v in state_dict.items() if \"lora\" in k.lower()}\n",
    "                \n",
    "                    # ✅ Extract only LoRA weights from UNet and text encoder\n",
    "                    unet_lora = extract_lora_weights(accelerator.unwrap_model(unet).state_dict())\n",
    "                    text_lora = extract_lora_weights(accelerator.unwrap_model(pipe.text_encoder).state_dict())\n",
    "                \n",
    "                    # ✅ Combine weights into a single dictionary\n",
    "                    combined_lora = {**unet_lora, **text_lora}\n",
    "                \n",
    "                    # ✅ Save using safetensors\n",
    "                    save_file(combined_lora, os.path.join(save_path, \"lora_only.safetensors\"))\n",
    "                \n",
    "                    print(f\"✅ Saved combined LoRA checkpoint at step {global_step} → {save_path}/lora_only.safetensors\")\n",
    "\n",
    "                    \n",
    "                # 🔍 Generate sample image\n",
    "                if global_step % cfg.generate_every_n_steps == 0:\n",
    "                    img_path = os.path.join('/kaggle/working/gen_images', f\"step_{global_step}\")\n",
    "                    generate_sample_image(global_step, img_path)\n",
    "\n",
    "        # ✅ Exit condition\n",
    "        if global_step >= cfg.max_train_steps:\n",
    "            break\n",
    "    if global_step >= cfg.max_train_steps:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for saving both phase 1 weights at the end of the training into a single file. It contains unet, text_encoder weights into a single lora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Final save\n",
    "# # Save both UNet and text encoder\n",
    "# unet_state_dict = accelerator.unwrap_model(unet).state_dict()\n",
    "# text_encoder_state_dict = accelerator.unwrap_model(pipe.text_encoder).state_dict()\n",
    "\n",
    "# # Combine and save as safetensors\n",
    "# save_file({**unet_state_dict, **text_encoder_state_dict}, \"full_model_lora.safetensors\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7772174,
     "sourceId": 12336375,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
