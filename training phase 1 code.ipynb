{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DreamBooth + LoRA Training Pipeline for Realistic Vision V5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-12T17:17:07.248115Z",
     "iopub.status.busy": "2025-07-12T17:17:07.247542Z",
     "iopub.status.idle": "2025-07-12T17:17:50.167375Z",
     "shell.execute_reply": "2025-07-12T17:17:50.166820Z",
     "shell.execute_reply.started": "2025-07-12T17:17:07.248090Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 17:17:33.358642: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752340653.714298      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752340653.820441      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from diffusers import StableDiffusionPipeline, UNet2DConditionModel\n",
    "from diffusers.models.attention_processor import LoRAAttnProcessor\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T17:17:50.169657Z",
     "iopub.status.busy": "2025-07-12T17:17:50.169175Z",
     "iopub.status.idle": "2025-07-12T17:17:50.173491Z",
     "shell.execute_reply": "2025-07-12T17:17:50.172701Z",
     "shell.execute_reply.started": "2025-07-12T17:17:50.169638Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T17:17:50.174372Z",
     "iopub.status.busy": "2025-07-12T17:17:50.174165Z",
     "iopub.status.idle": "2025-07-12T17:17:50.353054Z",
     "shell.execute_reply": "2025-07-12T17:17:50.352501Z",
     "shell.execute_reply.started": "2025-07-12T17:17:50.174356Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# HF TOKEN\n",
    "from huggingface_hub import login\n",
    "login(\"your_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T17:17:50.354586Z",
     "iopub.status.busy": "2025-07-12T17:17:50.353950Z",
     "iopub.status.idle": "2025-07-12T17:18:08.858822Z",
     "shell.execute_reply": "2025-07-12T17:18:08.858204Z",
     "shell.execute_reply.started": "2025-07-12T17:17:50.354546Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af00b8d5e9aa4e0caa73fcd7f9a76a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Realistic_Vision_V5.1.safetensors:   0%|          | 0.00/4.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Downloading Realistic Vision V5\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "\n",
    "model_dir = \"/kaggle/working//base_model\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "ckpt_path = hf_hub_download(\n",
    "    repo_id=\"SG161222/Realistic_Vision_V5.1_noVAE\",\n",
    "    filename=\"Realistic_Vision_V5.1.safetensors\",\n",
    "    local_dir=model_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T17:18:08.860159Z",
     "iopub.status.busy": "2025-07-12T17:18:08.859879Z",
     "iopub.status.idle": "2025-07-12T17:18:22.128683Z",
     "shell.execute_reply": "2025-07-12T17:18:22.127840Z",
     "shell.execute_reply.started": "2025-07-12T17:18:08.860132Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-12 17:18:08--  https://raw.githubusercontent.com/CompVis/stable-diffusion/main/configs/stable-diffusion/v1-inference.yaml\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1873 (1.8K) [text/plain]\n",
      "Saving to: ‘v1-inference.yaml’\n",
      "\n",
      "v1-inference.yaml   100%[===================>]   1.83K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-07-12 17:18:09 (19.2 MB/s) - ‘v1-inference.yaml’ saved [1873/1873]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Downloading v1-inference.yaml\n",
    "!wget -O v1-inference.yaml https://raw.githubusercontent.com/CompVis/stable-diffusion/main/configs/stable-diffusion/v1-inference.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T17:18:22.130290Z",
     "iopub.status.busy": "2025-07-12T17:18:22.129944Z",
     "iopub.status.idle": "2025-07-12T17:18:41.507374Z",
     "shell.execute_reply": "2025-07-12T17:18:41.506442Z",
     "shell.execute_reply.started": "2025-07-12T17:18:22.130232Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334cf449dbad4eb491e5151a60daaa16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31172f991c6546cf9a303bcb65b45eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5ae5f85bb44d3e941e6cd18bcbd377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea470e4409e4af0b87632e4186eb258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a90707837634d29acfc9cff27757bdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08beb741d3674415b7a3381d43826231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6cac10dd4124050b843904ff1222123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca7e64be9ad4e4ab21759cab08a3a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2a5e7bd414441d8b00ce2c34e9d957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a674ecb4837a4040984bb27ea57f7d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/342 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/clip/feature_extraction_clip.py:30: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to /kaggle/working/realistic_vision_diffusers\n"
     ]
    }
   ],
   "source": [
    "# Converting to diffusers format\n",
    "from diffusers.pipelines.stable_diffusion.convert_from_ckpt import download_from_original_stable_diffusion_ckpt\n",
    "\n",
    "safetensors_path = \"/kaggle/working/base_model/Realistic_Vision_V5.1.safetensors\"\n",
    "output_dir = \"/kaggle/working/realistic_vision_diffusers\"\n",
    "\n",
    "converted_pipeline = download_from_original_stable_diffusion_ckpt(\n",
    "    safetensors_path,\n",
    "    \"/kaggle/working/v1-inference.yaml\",  # Must match SD1.5 or SD2.x\n",
    "    from_safetensors=True,\n",
    "    extract_ema=True,\n",
    "    device=\"cuda\"  # or \"cpu\"\n",
    ")\n",
    "\n",
    "# saving\n",
    "converted_pipeline.save_pretrained(output_dir)\n",
    "print(f\"Model saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining configurations for the training**\n",
    "\n",
    "trigger_word: This let's you call your character. Whenever you use this it means that you are calling/referring your character.\n",
    "\n",
    "batch_size: how many images it will train in each step\n",
    "\n",
    "gradient_accumulation: Using this I can increase my batch size with same GPU memory usage.\n",
    "\n",
    "effective batch size = (batch_size) x (gradient_accumulation)\n",
    "\n",
    "learning_rate: how fast you want the model to learn the character (too high the model might overfit, too low the model would not learn your character fully) (effective proven and tested learning rate would be from 5e-5 to 1e-6 and depending upon the training steps and dataset variation) \n",
    "\n",
    "max_train_steps: how many steps you want the training to run (1 step = 1 batch run)\n",
    "\n",
    "train_text_encoder: whether we want to train train_text_encoder or not. (if yes we would update weights of train text encoder used in the base model using our setup) (Note: training this is necessary when training on specific characters as the base model might struggle learning your trigger_word and how it is in different scenarios)\n",
    "\n",
    "lr_scheduler: how do you want your learning rate to decay. (I tried constant, no lr scheduler it doesn't work, it model overfitted or underfitted, cosine worked best for me as the learning_rate would decay slowly.)\n",
    "\n",
    "lr_warmup_steps: when you want your learning rate to start decay (e.g. 100 means leraning rate would start decaying after 100 steps with the lr_scheduler)\n",
    "\n",
    "lora_r: Determines the rank of the low-rank adaptation matrices, controlling the number of trainable parameters added for fine-tuning.\n",
    "\n",
    "lora_alpha: A scaling factor applied to the LoRA updates, adjusting their overall impact on the model's weights.\n",
    "\n",
    "lora_dropout: Probability of randomly dropping LoRA adaptation connections during training, which helps prevent overfitting by adding regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T17:18:41.509045Z",
     "iopub.status.busy": "2025-07-12T17:18:41.508724Z",
     "iopub.status.idle": "2025-07-12T17:18:41.515584Z",
     "shell.execute_reply": "2025-07-12T17:18:41.514564Z",
     "shell.execute_reply.started": "2025-07-12T17:18:41.509015Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # ✅ Paths\n",
    "    model_path = \"/kaggle/working/realistic_vision_diffusers\"\n",
    "    dataset_dir = \"/kaggle/input/gwen-phase1\"\n",
    "    output_dir = \"/kaggle/working/lora_gwen\"\n",
    "\n",
    "    # ✅ Training Behavior\n",
    "    trigger_word = \"sks\"  # Keep this for identity consistency\n",
    "    resolution = 512       # Good middle ground for face detail\n",
    "    \n",
    "    batch_size = 1\n",
    "    gradient_accumulation = 4\n",
    "    learning_rate = 4e-5   # \n",
    "    max_train_steps = 3330 # \n",
    "    \n",
    "    mixed_precision = \"fp16\"\n",
    "    train_text_encoder = True  \n",
    "\n",
    "    # ✅ LoRA Settings\n",
    "    lr_scheduler = \"cosine\"\n",
    "    lr_warmup_steps = 185\n",
    "    lora_r = 4\n",
    "    lora_alpha = 8\n",
    "    lora_dropout = 0.1\n",
    "    # lora_target_modules = [\"CrossAttention\", \"Attention\"]\n",
    "\n",
    "    # ✅ Logging & Checkpoints\n",
    "    save_every_n_steps = 1110\n",
    "    log_every_n_steps = 370\n",
    "    generate_every_n_steps = 370\n",
    "    seed = 151101\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T17:18:41.519883Z",
     "iopub.status.busy": "2025-07-12T17:18:41.519681Z",
     "iopub.status.idle": "2025-07-12T17:18:57.054084Z",
     "shell.execute_reply": "2025-07-12T17:18:57.052992Z",
     "shell.execute_reply.started": "2025-07-12T17:18:41.519867Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  Config saved to /kaggle/working/lora_gwen/lora_config.json\n"
     ]
    }
   ],
   "source": [
    "import os, json, inspect\n",
    "\n",
    "def save_config(cfg, path=None):\n",
    "    if path is None:\n",
    "        path = os.path.join(cfg.output_dir, \"lora_config.json\")\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "    # grab only data attributes declared on the *class*\n",
    "    config_dict = {\n",
    "        k: v for k, v in cfg.__class__.__dict__.items()\n",
    "        if not k.startswith(\"__\") and not inspect.isfunction(v) and not inspect.ismethod(v)\n",
    "    }\n",
    "\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(config_dict, f, indent=4)\n",
    "    print(f\"✅  Config saved to {path}\")\n",
    "\n",
    "save_config(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T17:18:57.055307Z",
     "iopub.status.busy": "2025-07-12T17:18:57.055051Z",
     "iopub.status.idle": "2025-07-12T17:18:58.588672Z",
     "shell.execute_reply": "2025-07-12T17:18:58.587785Z",
     "shell.execute_reply.started": "2025-07-12T17:18:57.055266Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# @title Reproducibility\n",
    "torch.manual_seed(cfg.seed)\n",
    "torch.cuda.manual_seed(cfg.seed)\n",
    "random.seed(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T17:18:58.589599Z",
     "iopub.status.busy": "2025-07-12T17:18:58.589380Z",
     "iopub.status.idle": "2025-07-12T17:19:00.047591Z",
     "shell.execute_reply": "2025-07-12T17:19:00.046893Z",
     "shell.execute_reply.started": "2025-07-12T17:18:58.589582Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#  Dataset Loader\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, image_dir, tokenizer, size):\n",
    "        self.image_paths = []\n",
    "        self.caption_paths = []\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        for fname in sorted(os.listdir(image_dir)):\n",
    "            if fname.endswith(\".png\") or fname.endswith(\".jpg\"):\n",
    "                img_path = os.path.join(image_dir, fname)\n",
    "                txt_path = os.path.splitext(img_path)[0] + \".txt\"\n",
    "                if os.path.exists(txt_path):\n",
    "                    self.image_paths.append(img_path)\n",
    "                    self.caption_paths.append(txt_path)\n",
    "\n",
    "        self.image_transforms = transforms.Compose([\n",
    "            transforms.Resize((size, size), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        if image.getbbox() is None:\n",
    "            raise ValueError(f\"Empty image found: {self.image_paths[idx]}\")\n",
    "        else:\n",
    "            image = self.image_transforms(image)\n",
    "            with open(self.caption_paths[idx], \"r\") as f:\n",
    "                caption = f.read().strip()\n",
    "\n",
    "            inputs = self.tokenizer(caption, truncation=True, padding=\"max_length\", max_length=77, return_tensors=\"pt\")\n",
    "            \n",
    "        return {\"pixel_values\": image, \"input_ids\": inputs.input_ids.squeeze(0)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was frustrated by version mismatches and compatibility issues with popular libraries (like peft) for LoRA in pytorch. Instead of fighting package dependencies, I researched the fundamental principles behind LoRA and wrote a minimal, robust implementation compatible with any pytorch model using nn.Linear layers.\n",
    "\n",
    "My LoRALinear class is a simple, direct wrapper for any nn.Linear layer. It adds trainable “LoRA” weights on top of the original weights, enabling efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T17:19:00.048822Z",
     "iopub.status.busy": "2025-07-12T17:19:00.048560Z",
     "iopub.status.idle": "2025-07-12T17:19:00.917728Z",
     "shell.execute_reply": "2025-07-12T17:19:00.917090Z",
     "shell.execute_reply.started": "2025-07-12T17:19:00.048803Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# 🔧 Safer and more flexible LoRA wrapper for nn.Linear\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, linear: nn.Linear, rank: int, alpha: float):\n",
    "        super().__init__()\n",
    "\n",
    "        if not isinstance(linear, nn.Linear):\n",
    "            raise TypeError(f\"LoRALinear can only wrap nn.Linear, but got {type(linear)}\")\n",
    "\n",
    "        self.linear = linear\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "\n",
    "        # LoRA layers\n",
    "        self.lora_down = nn.Linear(linear.in_features, rank, bias=False)\n",
    "        self.lora_up = nn.Linear(rank, linear.out_features, bias=False)\n",
    "\n",
    "        # Initialization (standard LoRA practice)\n",
    "        nn.init.kaiming_uniform_(self.lora_down.weight, a=5**0.5)\n",
    "        nn.init.zeros_(self.lora_up.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Original + LoRA residual\n",
    "        return self.linear(x) + self.lora_up(self.lora_down(x)) * self.scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our patcher for patching unet.\n",
    "\n",
    "The most important question what are we patching, why that part and we would we patch?\n",
    "\n",
    "Answer: We would patch some layers of the model not all, the layers most important for learning, new style, new character, new things are \"to_q\" ,\"to_k\", \"to_v\" and \"to_out\". These layers work together when you train your model they contain their own weights for what they learned. If we patched the old weights we might mess with the working of the model and it may produce messy, or noisy results during image generation. That's why we patch new layers and train the model these contain new weights for your specific style, character and new things. And you can load these on top of your Stable Diffusion Model and use them for image generation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T17:19:00.919023Z",
     "iopub.status.busy": "2025-07-12T17:19:00.918743Z",
     "iopub.status.idle": "2025-07-12T17:19:00.941772Z",
     "shell.execute_reply": "2025-07-12T17:19:00.941058Z",
     "shell.execute_reply.started": "2025-07-12T17:19:00.919003Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def patch_unet_cross_attn_with_lora(unet, rank, alpha):\n",
    "    lora_params = []\n",
    "\n",
    "    for module in unet.modules():\n",
    "        for attr in ['to_q', 'to_k', 'to_v', 'to_out']:\n",
    "            if hasattr(module, attr):\n",
    "                original = getattr(module, attr)\n",
    "\n",
    "                # ✅ Directly patch if it's a Linear\n",
    "                if isinstance(original, nn.Linear) and not isinstance(original, LoRALinear):\n",
    "                    lora_layer = LoRALinear(original, rank=rank, alpha=alpha)\n",
    "                    setattr(module, attr, lora_layer)\n",
    "                    lora_params.extend(lora_layer.lora_down.parameters())\n",
    "                    lora_params.extend(lora_layer.lora_up.parameters())\n",
    "                    print(f\"✅ Patched {attr} in {module.__class__.__name__}\")\n",
    "\n",
    "                # 🔍 Special case: to_out is a ModuleList with a Linear inside\n",
    "                elif isinstance(original, nn.ModuleList):\n",
    "                    for i, sublayer in enumerate(original):\n",
    "                        if isinstance(sublayer, nn.Linear) and not isinstance(sublayer, LoRALinear):\n",
    "                            lora_layer = LoRALinear(sublayer, rank=rank, alpha=alpha)\n",
    "                            original[i] = lora_layer\n",
    "                            lora_params.extend(lora_layer.lora_down.parameters())\n",
    "                            lora_params.extend(lora_layer.lora_up.parameters())\n",
    "                            print(f\"✅ Patched {attr}[{i}] in {module.__class__.__name__}\")\n",
    "                        else:\n",
    "                            print(f\"⚠️ Skipping {attr}[{i}] — Not a Linear: {type(sublayer)}\")\n",
    "\n",
    "                else:\n",
    "                    print(f\"⚠️ Skipping {attr} in {module.__class__.__name__} — Not a Linear or ModuleList\")\n",
    "\n",
    "    total_params = sum(p.numel() for p in lora_params)\n",
    "    print(f\"\\n✅ UNet LoRA patched. Total trainable LoRA params: {total_params}\")\n",
    "    return lora_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is to unfrreze train_text encoder. I defined it for testing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T17:19:00.943304Z",
     "iopub.status.busy": "2025-07-12T17:19:00.942705Z",
     "iopub.status.idle": "2025-07-12T17:19:00.965620Z",
     "shell.execute_reply": "2025-07-12T17:19:00.964810Z",
     "shell.execute_reply.started": "2025-07-12T17:19:00.943257Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def unfreeze_text_encoder_attention(text_encoder):\n",
    "#     trainable_params = []\n",
    "#     total_unfroze = 0\n",
    "\n",
    "#     for module in text_encoder.modules():\n",
    "#         if all(hasattr(module, attr) for attr in ['q_proj', 'k_proj', 'v_proj', 'out_proj']):\n",
    "#             for attr in ['q_proj', 'k_proj', 'v_proj', 'out_proj']:\n",
    "#                 proj = getattr(module, attr)\n",
    "#                 for param in proj.parameters():\n",
    "#                     param.requires_grad = True\n",
    "#                     trainable_params.append(param)\n",
    "#                 total_unfroze += 1\n",
    "#                 print(f\"✅ Unfroze text encoder layer: {module.__class__.__name__} → {attr}\")\n",
    "\n",
    "#     print(f\"\\n✅ Total unfrozen attention projection blocks in text encoder: {total_unfroze}\")\n",
    "#     return trainable_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will patch the text_encoder the theory and reason is same as patching the unet layers. We would patch \"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\" of the text_encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T17:19:00.966846Z",
     "iopub.status.busy": "2025-07-12T17:19:00.966573Z",
     "iopub.status.idle": "2025-07-12T17:19:00.989979Z",
     "shell.execute_reply": "2025-07-12T17:19:00.989234Z",
     "shell.execute_reply.started": "2025-07-12T17:19:00.966818Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def patch_text_encoder(text_encoder, rank, alpha):\n",
    "    lora_params = []\n",
    "    target_names = {\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"}\n",
    "\n",
    "    for module in text_encoder.modules():\n",
    "        for name in target_names:\n",
    "            if hasattr(module, name):\n",
    "                proj = getattr(module, name)\n",
    "\n",
    "                # Only patch raw nn.Linear layers (skip if already patched)\n",
    "                if isinstance(proj, nn.Linear) and not isinstance(proj, LoRALinear):\n",
    "                    lora_layer = LoRALinear(proj, rank=rank, alpha=alpha)\n",
    "                    setattr(module, name, lora_layer)\n",
    "\n",
    "                    lora_params += list(lora_layer.lora_down.parameters())\n",
    "                    lora_params += list(lora_layer.lora_up.parameters())\n",
    "\n",
    "                    print(f\"🔧 Patched {module.__class__.__name__}.{name} with LoRA\")\n",
    "\n",
    "    print(f\"✅ Finished patching text encoder — LoRA params: {sum(p.numel() for p in lora_params):,}\")\n",
    "    return lora_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this is where we would create our pipeline, load our model into pipeline, load our tokenizer, load our trigger word into the tokenizer, load our unet and patch it, load our text_encoder and patch it. But the important thing is that we need to freeze our params of unet and text_encoder before patching so we accidently don't train them or update them and we only train or update our newly patched layers. We are using AdamW optimer here originally used with base our base model. We would also load our dataset into the pipeline and prepare everything for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T17:19:00.991063Z",
     "iopub.status.busy": "2025-07-12T17:19:00.990835Z",
     "iopub.status.idle": "2025-07-12T17:19:09.637759Z",
     "shell.execute_reply": "2025-07-12T17:19:09.636638Z",
     "shell.execute_reply.started": "2025-07-12T17:19:00.991036Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d34bc677254044ae83cde8634d8511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/clip/feature_extraction_clip.py:30: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Added custom token 'pendugwen' (id 49408)\n",
      "✅ Patched to_q in Attention\n",
      "✅ Patched to_k in Attention\n",
      "✅ Patched to_v in Attention\n",
      "✅ Patched to_out[0] in Attention\n",
      "⚠️ Skipping to_out[1] — Not a Linear: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "✅ Patched to_q in Attention\n",
      "✅ Patched to_k in Attention\n",
      "✅ Patched to_v in Attention\n",
      "✅ Patched to_out[0] in Attention\n",
      "⚠️ Skipping to_out[1] — Not a Linear: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "✅ Patched to_q in Attention\n",
      "✅ Patched to_k in Attention\n",
      "✅ Patched to_v in Attention\n",
      "✅ Patched to_out[0] in Attention\n",
      "⚠️ Skipping to_out[1] — Not a Linear: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "✅ Patched to_q in Attention\n",
      "✅ Patched to_k in Attention\n",
      "✅ Patched to_v in Attention\n",
      "✅ Patched to_out[0] in Attention\n",
      "⚠️ Skipping to_out[1] — Not a Linear: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "✅ Patched to_q in Attention\n",
      "✅ Patched to_k in Attention\n",
      "✅ Patched to_v in Attention\n",
      "✅ Patched to_out[0] in Attention\n",
      "⚠️ Skipping to_out[1] — Not a Linear: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "✅ Patched to_q in Attention\n",
      "✅ Patched to_k in Attention\n",
      "✅ Patched to_v in Attention\n",
      "✅ Patched to_out[0] in Attention\n",
      "⚠️ Skipping to_out[1] — Not a Linear: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "✅ Patched to_q in Attention\n",
      "✅ Patched to_k in Attention\n",
      "✅ Patched to_v in Attention\n",
      "✅ Patched to_out[0] in Attention\n",
      "⚠️ Skipping to_out[1] — Not a Linear: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "✅ Patched to_q in Attention\n",
      "✅ Patched to_k in Attention\n",
      "✅ Patched to_v in Attention\n",
      "✅ Patched to_out[0] in Attention\n",
      "⚠️ Skipping to_out[1] — Not a Linear: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "✅ Patched to_q in Attention\n",
      "✅ Patched to_k in Attention\n",
      "✅ Patched to_v in Attention\n",
      "✅ Patched to_out[0] in Attention\n",
      "⚠️ Skipping to_out[1] — Not a Linear: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "✅ Patched to_q in Attention\n",
      "✅ Patched to_k in Attention\n",
      "✅ Patched to_v in Attention\n",
      "✅ Patched to_out[0] in Attention\n",
      "⚠️ Skipping to_out[1] — Not a Linear: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "✅ Patched to_q in Attention\n",
      "✅ Patched to_k in Attention\n",
      "✅ Patched to_v in Attention\n",
      "✅ Patched to_out[0] in Attention\n",
      "⚠️ Skipping to_out[1] — Not a Linear: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "✅ Patched to_q in Attention\n",
      "✅ Patched to_k in Attention\n",
      "✅ Patched to_v in Attention\n",
      "✅ Patched to_out[0] in Attention\n",
      "⚠️ Skipping to_out[1] — Not a Linear: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "✅ Patched to_q in Attention\n",
      "✅ Patched to_k in Attention\n",
      "✅ Patched to_v in Attention\n",
      "✅ Patched to_out[0] in Attention\n",
      "⚠️ Skipping to_out[1] — Not a Linear: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "✅ Patched to_q in Attention\n",
      "✅ Patched to_k in Attention\n",
      "✅ Patched to_v in Attention\n",
      "✅ Patched to_out[0] in Attention\n",
      "⚠️ Skipping to_out[1] — Not a Linear: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "✅ Patched to_q in Attention\n",
      "✅ Patched to_k in Attention\n",
      "✅ Patched to_v in Attention\n",
      "✅ Patched to_out[0] in Attention\n",
      "⚠️ Skipping to_out[1] — Not a Linear: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "✅ Patched to_q in Attention\n",
      "✅ Patched to_k in Attention\n",
      "✅ Patched to_v in Attention\n",
      "✅ Patched to_out[0] in Attention\n",
      "⚠️ Skipping to_out[1] — Not a Linear: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "✅ Patched to_q in Attention\n",
      "✅ Patched to_k in Attention\n",
      "✅ Patched to_v in Attention\n",
      "✅ Patched to_out[0] in Attention\n",
      "⚠️ Skipping to_out[1] — Not a Linear: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "✅ Patched to_q in Attention\n",
      "✅ Patched to_k in Attention\n",
      "✅ Patched to_v in Attention\n",
      "✅ Patched to_out[0] in Attention\n",
      "⚠️ Skipping to_out[1] — Not a Linear: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "✅ Patched to_q in Attention\n",
      "✅ Patched to_k in Attention\n",
      "✅ Patched to_v in Attention\n",
      "✅ Patched to_out[0] in Attention\n",
      "⚠️ Skipping to_out[1] — Not a Linear: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "✅ Patched to_q in Attention\n",
      "✅ Patched to_k in Attention\n",
      "✅ Patched to_v in Attention\n",
      "✅ Patched to_out[0] in Attention\n",
      "⚠️ Skipping to_out[1] — Not a Linear: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "✅ Patched to_q in Attention\n",
      "✅ Patched to_k in Attention\n",
      "✅ Patched to_v in Attention\n",
      "✅ Patched to_out[0] in Attention\n",
      "⚠️ Skipping to_out[1] — Not a Linear: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "✅ Patched to_q in Attention\n",
      "✅ Patched to_k in Attention\n",
      "✅ Patched to_v in Attention\n",
      "✅ Patched to_out[0] in Attention\n",
      "⚠️ Skipping to_out[1] — Not a Linear: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "✅ Patched to_q in Attention\n",
      "✅ Patched to_k in Attention\n",
      "✅ Patched to_v in Attention\n",
      "✅ Patched to_out[0] in Attention\n",
      "⚠️ Skipping to_out[1] — Not a Linear: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "✅ Patched to_q in Attention\n",
      "✅ Patched to_k in Attention\n",
      "✅ Patched to_v in Attention\n",
      "✅ Patched to_out[0] in Attention\n",
      "⚠️ Skipping to_out[1] — Not a Linear: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "✅ Patched to_q in Attention\n",
      "✅ Patched to_k in Attention\n",
      "✅ Patched to_v in Attention\n",
      "✅ Patched to_out[0] in Attention\n",
      "⚠️ Skipping to_out[1] — Not a Linear: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "✅ Patched to_q in Attention\n",
      "✅ Patched to_k in Attention\n",
      "✅ Patched to_v in Attention\n",
      "✅ Patched to_out[0] in Attention\n",
      "⚠️ Skipping to_out[1] — Not a Linear: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "✅ Patched to_q in Attention\n",
      "✅ Patched to_k in Attention\n",
      "✅ Patched to_v in Attention\n",
      "✅ Patched to_out[0] in Attention\n",
      "⚠️ Skipping to_out[1] — Not a Linear: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "✅ Patched to_q in Attention\n",
      "✅ Patched to_k in Attention\n",
      "✅ Patched to_v in Attention\n",
      "✅ Patched to_out[0] in Attention\n",
      "⚠️ Skipping to_out[1] — Not a Linear: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "✅ Patched to_q in Attention\n",
      "✅ Patched to_k in Attention\n",
      "✅ Patched to_v in Attention\n",
      "✅ Patched to_out[0] in Attention\n",
      "⚠️ Skipping to_out[1] — Not a Linear: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "✅ Patched to_q in Attention\n",
      "✅ Patched to_k in Attention\n",
      "✅ Patched to_v in Attention\n",
      "✅ Patched to_out[0] in Attention\n",
      "⚠️ Skipping to_out[1] — Not a Linear: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "✅ Patched to_q in Attention\n",
      "✅ Patched to_k in Attention\n",
      "✅ Patched to_v in Attention\n",
      "✅ Patched to_out[0] in Attention\n",
      "⚠️ Skipping to_out[1] — Not a Linear: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "✅ Patched to_q in Attention\n",
      "✅ Patched to_k in Attention\n",
      "✅ Patched to_v in Attention\n",
      "✅ Patched to_out[0] in Attention\n",
      "⚠️ Skipping to_out[1] — Not a Linear: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "\n",
      "✅ UNet LoRA patched. Total trainable LoRA params: 797184\n",
      "🔧 Patched CLIPAttention.q_proj with LoRA\n",
      "🔧 Patched CLIPAttention.out_proj with LoRA\n",
      "🔧 Patched CLIPAttention.v_proj with LoRA\n",
      "🔧 Patched CLIPAttention.k_proj with LoRA\n",
      "🔧 Patched CLIPAttention.q_proj with LoRA\n",
      "🔧 Patched CLIPAttention.out_proj with LoRA\n",
      "🔧 Patched CLIPAttention.v_proj with LoRA\n",
      "🔧 Patched CLIPAttention.k_proj with LoRA\n",
      "🔧 Patched CLIPAttention.q_proj with LoRA\n",
      "🔧 Patched CLIPAttention.out_proj with LoRA\n",
      "🔧 Patched CLIPAttention.v_proj with LoRA\n",
      "🔧 Patched CLIPAttention.k_proj with LoRA\n",
      "🔧 Patched CLIPAttention.q_proj with LoRA\n",
      "🔧 Patched CLIPAttention.out_proj with LoRA\n",
      "🔧 Patched CLIPAttention.v_proj with LoRA\n",
      "🔧 Patched CLIPAttention.k_proj with LoRA\n",
      "🔧 Patched CLIPAttention.q_proj with LoRA\n",
      "🔧 Patched CLIPAttention.out_proj with LoRA\n",
      "🔧 Patched CLIPAttention.v_proj with LoRA\n",
      "🔧 Patched CLIPAttention.k_proj with LoRA\n",
      "🔧 Patched CLIPAttention.q_proj with LoRA\n",
      "🔧 Patched CLIPAttention.out_proj with LoRA\n",
      "🔧 Patched CLIPAttention.v_proj with LoRA\n",
      "🔧 Patched CLIPAttention.k_proj with LoRA\n",
      "🔧 Patched CLIPAttention.q_proj with LoRA\n",
      "🔧 Patched CLIPAttention.out_proj with LoRA\n",
      "🔧 Patched CLIPAttention.v_proj with LoRA\n",
      "🔧 Patched CLIPAttention.k_proj with LoRA\n",
      "🔧 Patched CLIPAttention.q_proj with LoRA\n",
      "🔧 Patched CLIPAttention.out_proj with LoRA\n",
      "🔧 Patched CLIPAttention.v_proj with LoRA\n",
      "🔧 Patched CLIPAttention.k_proj with LoRA\n",
      "🔧 Patched CLIPAttention.q_proj with LoRA\n",
      "🔧 Patched CLIPAttention.out_proj with LoRA\n",
      "🔧 Patched CLIPAttention.v_proj with LoRA\n",
      "🔧 Patched CLIPAttention.k_proj with LoRA\n",
      "🔧 Patched CLIPAttention.q_proj with LoRA\n",
      "🔧 Patched CLIPAttention.out_proj with LoRA\n",
      "🔧 Patched CLIPAttention.v_proj with LoRA\n",
      "🔧 Patched CLIPAttention.k_proj with LoRA\n",
      "🔧 Patched CLIPAttention.q_proj with LoRA\n",
      "🔧 Patched CLIPAttention.out_proj with LoRA\n",
      "🔧 Patched CLIPAttention.v_proj with LoRA\n",
      "🔧 Patched CLIPAttention.k_proj with LoRA\n",
      "🔧 Patched CLIPAttention.q_proj with LoRA\n",
      "🔧 Patched CLIPAttention.out_proj with LoRA\n",
      "🔧 Patched CLIPAttention.v_proj with LoRA\n",
      "🔧 Patched CLIPAttention.k_proj with LoRA\n",
      "✅ Finished patching text encoder — LoRA params: 294,912\n",
      "✅ Text‑encoder LoRA patched\n",
      "🔍 LoRA trainable parameters: 1,092,096\n",
      "🚀 Setup complete – ready to train LoRA adapters.\n"
     ]
    }
   ],
   "source": [
    "import torch, os\n",
    "from accelerate import Accelerator\n",
    "from diffusers          import StableDiffusionPipeline, UNet2DConditionModel\n",
    "from transformers        import AutoTokenizer, get_scheduler\n",
    "from torch.utils.data    import DataLoader\n",
    "\n",
    "# ──────────────────────────────────────────────────────────\n",
    "#  1.  Accelerate & Config\n",
    "# ──────────────────────────────────────────────────────────\n",
    "accelerator = Accelerator(split_batches=True)\n",
    "device       = accelerator.device\n",
    "cfg          = Config()                       # <- your existing config class\n",
    "\n",
    "# ──────────────────────────────────────────────────────────\n",
    "#  2.  Load full pipeline   (VAE, Text‑Encoder, UNet, etc.)\n",
    "# ──────────────────────────────────────────────────────────\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    cfg.model_path,\n",
    "    torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────\n",
    "#  3.  Trigger‑token → add BEFORE any dataset tokenisation\n",
    "# ──────────────────────────────────────────────────────────\n",
    "tokenizer      = AutoTokenizer.from_pretrained(cfg.model_path, subfolder=\"tokenizer\")\n",
    "trigger_token  = cfg.trigger_word                    # e.g. \"pendugwen\"\n",
    "\n",
    "if len(tokenizer.tokenize(trigger_token)) > 1:       # splits → need custom token\n",
    "    tokenizer.add_tokens([trigger_token])\n",
    "    pipe.text_encoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # initialise the new embedding from \"person\"\n",
    "    with torch.no_grad():\n",
    "        emb           = pipe.text_encoder.get_input_embeddings()\n",
    "        new_id        = tokenizer.convert_tokens_to_ids(trigger_token)\n",
    "        base_id       = tokenizer.convert_tokens_to_ids(\"person\")\n",
    "        emb.weight[new_id] = emb.weight[base_id].clone()\n",
    "\n",
    "    print(f\"✅ Added custom token '{trigger_token}' (id {new_id})\")\n",
    "else:\n",
    "    print(f\"✅ '{trigger_token}' already a single token\")\n",
    "\n",
    "pipe.tokenizer = tokenizer        # keep pipeline & dataset in sync\n",
    "\n",
    "# ──────────────────────────────────────────────────────────\n",
    "#  4.  Load UNet separately (so we can LoRA‑patch it)\n",
    "# ──────────────────────────────────────────────────────────\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    cfg.model_path,\n",
    "    subfolder=\"unet\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "# Freeze everything first\n",
    "for p in unet.parameters():            p.requires_grad = False\n",
    "for p in pipe.text_encoder.parameters(): p.requires_grad = False\n",
    "\n",
    "# ──────────────────────────────────────────────────────────\n",
    "#  5.  LoRA‑patch UNet  (+ optional text‑encoder)\n",
    "# ──────────────────────────────────────────────────────────\n",
    "lora_params  = patch_unet_cross_attn_with_lora(unet, cfg.lora_r, cfg.lora_alpha)\n",
    "\n",
    "if cfg.train_text_encoder:\n",
    "    lora_params += patch_text_encoder(pipe.text_encoder, cfg.lora_r, cfg.lora_alpha)\n",
    "    print(\"✅ Text‑encoder LoRA patched\")\n",
    "\n",
    "if not lora_params:\n",
    "    raise RuntimeError(\"No trainable LoRA params collected!\")\n",
    "\n",
    "print(f\"🔍 LoRA trainable parameters: {sum(p.numel() for p in lora_params):,}\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────\n",
    "#  6.  Optimiser & Scheduler (LoRA params only)\n",
    "# ──────────────────────────────────────────────────────────\n",
    "optimizer     = torch.optim.AdamW(lora_params, lr=cfg.learning_rate)\n",
    "\n",
    "lr_scheduler  = get_scheduler(\n",
    "    cfg.lr_scheduler,\n",
    "    optimizer          = optimizer,\n",
    "    num_warmup_steps   = cfg.lr_warmup_steps,\n",
    "    num_training_steps = cfg.max_train_steps,\n",
    ")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────\n",
    "#  7.  Dataset & Dataloader  (tokenizer now has fixed token!)\n",
    "# ──────────────────────────────────────────────────────────\n",
    "dataset    = ImageCaptionDataset(cfg.dataset_dir, tokenizer, cfg.resolution)\n",
    "dataloader = DataLoader(dataset, batch_size=cfg.batch_size, shuffle=True)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────\n",
    "#  8.  Prepare for Accelerate & training\n",
    "# ──────────────────────────────────────────────────────────\n",
    "pipe.text_encoder.to(device, dtype=torch.float16)\n",
    "unet.train()\n",
    "\n",
    "unet, optimizer, dataloader = accelerator.prepare(unet, optimizer, dataloader)\n",
    "\n",
    "print(\"🚀 Setup complete – ready to train LoRA adapters.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just check that how many params we are going to train now out of total params. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T17:19:09.646316Z",
     "iopub.status.busy": "2025-07-12T17:19:09.646057Z",
     "iopub.status.idle": "2025-07-12T17:19:09.679898Z",
     "shell.execute_reply": "2025-07-12T17:19:09.679191Z",
     "shell.execute_reply.started": "2025-07-12T17:19:09.646265Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Trainable parameters: 1092096 / 983674308\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in unet.parameters()) + sum(p.numel() for p in pipe.text_encoder.parameters())\n",
    "trainable_params = sum(p.numel() for p in unet.parameters() if p.requires_grad) + \\\n",
    "                   sum(p.numel() for p in pipe.text_encoder.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"✅ Trainable parameters: {trainable_params} / {total_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the vae used in the base model. The vae is what encodes and decodes our latent spaces into images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T17:19:09.681189Z",
     "iopub.status.busy": "2025-07-12T17:19:09.680918Z",
     "iopub.status.idle": "2025-07-12T17:19:13.640664Z",
     "shell.execute_reply": "2025-07-12T17:19:13.639851Z",
     "shell.execute_reply.started": "2025-07-12T17:19:09.681170Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "015c73ac6d23478b96f7d63a19485c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/547 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da45a04697354f24a2f7675af2607666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.safetensors:   0%|          | 0.00/335M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "AutoencoderKL(\n",
       "  (encoder): Encoder(\n",
       "    (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (down_blocks): ModuleList(\n",
       "      (0): DownEncoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): Downsample2D(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DownEncoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): Downsample2D(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): DownEncoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): Downsample2D(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DownEncoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mid_block): UNetMidBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0): Attention(\n",
       "          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "    (conv_act): SiLU()\n",
       "    (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (up_blocks): ModuleList(\n",
       "      (0-1): 2 x UpDecoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0-2): 3 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): Upsample2D(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): UpDecoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1-2): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): Upsample2D(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): UpDecoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1-2): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mid_block): UNetMidBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0): Attention(\n",
       "          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "    (conv_act): SiLU()\n",
       "    (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (post_quant_conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from diffusers import AutoencoderKL\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\", torch_dtype=torch.float32)\n",
    "pipe.vae = vae\n",
    "\n",
    "pipe.vae.to(accelerator.device, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T17:19:13.641828Z",
     "iopub.status.busy": "2025-07-12T17:19:13.641543Z",
     "iopub.status.idle": "2025-07-12T17:19:13.645895Z",
     "shell.execute_reply": "2025-07-12T17:19:13.645321Z",
     "shell.execute_reply.started": "2025-07-12T17:19:13.641807Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(accelerator.device)  # Should print: cuda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check that if our paramters contain NAN values before our training begins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T17:19:13.646969Z",
     "iopub.status.busy": "2025-07-12T17:19:13.646714Z",
     "iopub.status.idle": "2025-07-12T17:19:14.152888Z",
     "shell.execute_reply": "2025-07-12T17:19:14.152256Z",
     "shell.execute_reply.started": "2025-07-12T17:19:13.646946Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for name, param in unet.named_parameters():\n",
    "    if torch.isnan(param).any():\n",
    "        print(f\"NaN detected in UNet parameter: {name}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would create functions for checking if our trigger_word is in the tokenizer, a function to generate sample images during the model training for checks if our model is learning or not, whether it is overfitting or underfitting. Whether if we need to stop our training, whether we need to change our configurations settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T17:19:14.153998Z",
     "iopub.status.busy": "2025-07-12T17:19:14.153720Z",
     "iopub.status.idle": "2025-07-12T17:19:14.165191Z",
     "shell.execute_reply": "2025-07-12T17:19:14.164529Z",
     "shell.execute_reply.started": "2025-07-12T17:19:14.153976Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from diffusers import DDIMScheduler\n",
    "from torch import autocast\n",
    "\n",
    "# ✅ Optional: Reproducibility\n",
    "def seed_everything(seed=151101):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "# ✅ Optional: Disable NSFW filter (for local testing)\n",
    "def disable_safety(pipe):\n",
    "    pipe.safety_checker = lambda images, **kwargs: (images, [False] * len(images))\n",
    "\n",
    "# ✅ Fix tokenizer trigger word if needed\n",
    "def ensure_trigger_token(pipe, trigger_word):\n",
    "    tokens = pipe.tokenizer.tokenize(trigger_word)\n",
    "    if len(tokens) > 1:\n",
    "        print(f\"⚠️ Trigger word '{trigger_word}' is split: {tokens}. Fixing...\")\n",
    "        pipe.tokenizer.add_tokens([trigger_word])\n",
    "        pipe.text_encoder.resize_token_embeddings(len(pipe.tokenizer))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embeddings = pipe.text_encoder.get_input_embeddings()\n",
    "            init_id = pipe.tokenizer.convert_tokens_to_ids(\"person\")\n",
    "            new_id = pipe.tokenizer.convert_tokens_to_ids(trigger_word)\n",
    "            embeddings.weight[new_id] = embeddings.weight[init_id].clone()\n",
    "\n",
    "        print(f\"✅ Re-added and initialized embedding for trigger word '{trigger_word}'\")\n",
    "    else:\n",
    "        print(f\"✅ Trigger word '{trigger_word}' is tokenized correctly: {tokens}\")\n",
    "\n",
    "# ✅ Generate and save a sample image\n",
    "def generate_sample_image(step, save_path, prompt=None, negative_prompt=None, seed=151101):\n",
    "    print(f\"\\n🎨 Generating preview at step {step}...\")\n",
    "\n",
    "    # ✅ Reproducible randomness\n",
    "    seed_everything(seed)\n",
    "    generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
    "\n",
    "    # ✅ Restore UNet (EMA if available, else current)\n",
    "    if 'ema_unet' in globals() and ema_unet is not None:\n",
    "        pipe.unet = ema_unet\n",
    "        print(\"📦 Using EMA UNet for inference.\")\n",
    "    else:\n",
    "        pipe.unet = accelerator.unwrap_model(unet)\n",
    "        print(\"📦 Using current UNet for inference.\")\n",
    "    \n",
    "    # ✅ Restore LoRA-trained text_encoder if trained\n",
    "    if cfg.train_text_encoder:\n",
    "        pipe.text_encoder = accelerator.unwrap_model(pipe.text_encoder)\n",
    "        print(\"🧠 Restored LoRA-trained text encoder for inference.\")\n",
    "\n",
    "    pipe.to(\"cuda\")\n",
    "\n",
    "    # ✅ Disable NSFW checker for previewing\n",
    "    disable_safety(pipe)\n",
    "\n",
    "    # ✅ Ensure tokenizer supports the trigger word\n",
    "    ensure_trigger_token(pipe, cfg.trigger_word)\n",
    "\n",
    "    # ✅ Use fast scheduler\n",
    "    pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "    # ✅ Default prompts if not provided\n",
    "    if prompt is None:\n",
    "        prompt = f\"close-up of {cfg.trigger_word}, detailed facial features, natural skin texture, soft expression, white blonde hair, illuminated by natural light, shallow depth of field, blurred background, ultra high quality\"\n",
    "    if negative_prompt is None:\n",
    "        negative_prompt = (\n",
    "            \"blurry, low resolution, grainy, overexposed, underexposed, poor lighting, jpeg artifacts, glitch, \"\n",
    "            \"cropped, out of frame, watermark, duplicate, poorly drawn face, asymmetrical face, deformed face, \"\n",
    "            \"unnatural skin texture, doll-like face, bad eyes, mutated hands, extra fingers, distorted anatomy, \"\n",
    "            \"unrealistic proportions, cartoon, anime, illustration, painting, horror, morbid\"\n",
    "        )\n",
    "\n",
    "    # ✅ Generate image\n",
    "    with autocast(\"cuda\"):\n",
    "        # Generate 4 images\n",
    "        result = pipe(\n",
    "            prompt=[prompt] * 4,\n",
    "            negative_prompt=[negative_prompt] * 4,\n",
    "            num_inference_steps=30,\n",
    "            guidance_scale=6.0,\n",
    "            height=cfg.resolution,\n",
    "            width=cfg.resolution,\n",
    "            generator=generator,\n",
    "        )\n",
    "\n",
    "    # ✅ Save images\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    for i, image in enumerate(result.images):\n",
    "        save_name = os.path.join(save_path, f\"preview_step_{step}_{i+1}.png\")\n",
    "        image.save(save_name)\n",
    "        print(f\"✅ Saved: {save_name}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our main training loop for training our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T17:19:14.166262Z",
     "iopub.status.busy": "2025-07-12T17:19:14.166007Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:44<00:00,  1.20s/it]\n",
      "100%|██████████| 37/37 [00:44<00:00,  1.20s/it]\n",
      "100%|██████████| 37/37 [00:46<00:00,  1.27s/it]\n",
      "100%|██████████| 37/37 [00:45<00:00,  1.24s/it]\n",
      "100%|██████████| 37/37 [00:46<00:00,  1.25s/it]\n",
      "100%|██████████| 37/37 [00:46<00:00,  1.25s/it]\n",
      "100%|██████████| 37/37 [00:46<00:00,  1.25s/it]\n",
      "100%|██████████| 37/37 [00:46<00:00,  1.24s/it]\n",
      "100%|██████████| 37/37 [00:45<00:00,  1.24s/it]\n",
      " 97%|█████████▋| 36/37 [00:44<00:01,  1.24s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (89 > 77). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 370 | Loss: 0.5961\n",
      "\n",
      "🎨 Generating preview at step 370...\n",
      "📦 Using current UNet for inference.\n",
      "🧠 Restored LoRA-trained text encoder for inference.\n",
      "✅ Trigger word 'pendugwen' is tokenized correctly: ['pendugwen']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5846cc7546942bb8c9cdcc68911c380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: /kaggle/working/gen_images/step_370/preview_step_370_1.png\n",
      "✅ Saved: /kaggle/working/gen_images/step_370/preview_step_370_2.png\n",
      "✅ Saved: /kaggle/working/gen_images/step_370/preview_step_370_3.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [01:09<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: /kaggle/working/gen_images/step_370/preview_step_370_4.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:45<00:00,  1.24s/it]\n",
      "100%|██████████| 37/37 [00:45<00:00,  1.24s/it]\n",
      "100%|██████████| 37/37 [00:46<00:00,  1.25s/it]\n",
      "100%|██████████| 37/37 [00:45<00:00,  1.24s/it]\n",
      "100%|██████████| 37/37 [00:45<00:00,  1.24s/it]\n",
      "100%|██████████| 37/37 [00:45<00:00,  1.24s/it]\n",
      "100%|██████████| 37/37 [00:45<00:00,  1.24s/it]\n",
      "100%|██████████| 37/37 [00:46<00:00,  1.25s/it]\n",
      "100%|██████████| 37/37 [00:46<00:00,  1.25s/it]\n",
      " 97%|█████████▋| 36/37 [00:45<00:01,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 740 | Loss: 0.2361\n",
      "\n",
      "🎨 Generating preview at step 740...\n",
      "📦 Using current UNet for inference.\n",
      "🧠 Restored LoRA-trained text encoder for inference.\n",
      "✅ Trigger word 'pendugwen' is tokenized correctly: ['pendugwen']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6c33361a78742a6bdef26df7a865303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: /kaggle/working/gen_images/step_740/preview_step_740_1.png\n",
      "✅ Saved: /kaggle/working/gen_images/step_740/preview_step_740_2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [01:09<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: /kaggle/working/gen_images/step_740/preview_step_740_3.png\n",
      "✅ Saved: /kaggle/working/gen_images/step_740/preview_step_740_4.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:46<00:00,  1.25s/it]\n",
      "100%|██████████| 37/37 [00:46<00:00,  1.25s/it]\n",
      "100%|██████████| 37/37 [00:46<00:00,  1.25s/it]\n",
      "100%|██████████| 37/37 [00:46<00:00,  1.26s/it]\n",
      "100%|██████████| 37/37 [00:46<00:00,  1.26s/it]\n",
      "100%|██████████| 37/37 [00:46<00:00,  1.25s/it]\n",
      "100%|██████████| 37/37 [00:46<00:00,  1.26s/it]\n",
      "100%|██████████| 37/37 [00:46<00:00,  1.25s/it]\n",
      "100%|██████████| 37/37 [00:46<00:00,  1.25s/it]\n",
      " 97%|█████████▋| 36/37 [00:45<00:01,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1110 | Loss: 0.2345\n",
      "✅ Saved combined LoRA checkpoint at step 1110 → /kaggle/working/lora_gwen/step_1110/lora_only.safetensors\n",
      "\n",
      "🎨 Generating preview at step 1110...\n",
      "📦 Using current UNet for inference.\n",
      "🧠 Restored LoRA-trained text encoder for inference.\n",
      "✅ Trigger word 'pendugwen' is tokenized correctly: ['pendugwen']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c7d07f168147a287681032731d2ef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: /kaggle/working/gen_images/step_1110/preview_step_1110_1.png\n",
      "✅ Saved: /kaggle/working/gen_images/step_1110/preview_step_1110_2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [01:09<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: /kaggle/working/gen_images/step_1110/preview_step_1110_3.png\n",
      "✅ Saved: /kaggle/working/gen_images/step_1110/preview_step_1110_4.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:46<00:00,  1.25s/it]\n",
      "100%|██████████| 37/37 [00:46<00:00,  1.25s/it]\n",
      "100%|██████████| 37/37 [00:46<00:00,  1.25s/it]\n",
      "100%|██████████| 37/37 [00:46<00:00,  1.25s/it]\n",
      "100%|██████████| 37/37 [00:46<00:00,  1.25s/it]\n",
      "100%|██████████| 37/37 [00:46<00:00,  1.25s/it]\n",
      "100%|██████████| 37/37 [00:45<00:00,  1.24s/it]\n",
      "100%|██████████| 37/37 [00:45<00:00,  1.24s/it]\n",
      "100%|██████████| 37/37 [00:46<00:00,  1.25s/it]\n",
      " 97%|█████████▋| 36/37 [00:44<00:01,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1480 | Loss: 0.2336\n",
      "\n",
      "🎨 Generating preview at step 1480...\n",
      "📦 Using current UNet for inference.\n",
      "🧠 Restored LoRA-trained text encoder for inference.\n",
      "✅ Trigger word 'pendugwen' is tokenized correctly: ['pendugwen']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86d2ce3fe285448884e7aa46abe5aac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: /kaggle/working/gen_images/step_1480/preview_step_1480_1.png\n",
      "✅ Saved: /kaggle/working/gen_images/step_1480/preview_step_1480_2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [01:09<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: /kaggle/working/gen_images/step_1480/preview_step_1480_3.png\n",
      "✅ Saved: /kaggle/working/gen_images/step_1480/preview_step_1480_4.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:45<00:00,  1.24s/it]\n",
      "100%|██████████| 37/37 [00:46<00:00,  1.24s/it]\n",
      "100%|██████████| 37/37 [00:46<00:00,  1.25s/it]\n",
      "100%|██████████| 37/37 [00:46<00:00,  1.25s/it]\n",
      "100%|██████████| 37/37 [00:46<00:00,  1.25s/it]\n",
      "100%|██████████| 37/37 [00:46<00:00,  1.25s/it]\n",
      "100%|██████████| 37/37 [00:46<00:00,  1.25s/it]\n",
      "100%|██████████| 37/37 [00:46<00:00,  1.25s/it]\n",
      "100%|██████████| 37/37 [00:46<00:00,  1.25s/it]\n",
      " 97%|█████████▋| 36/37 [00:45<00:01,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1850 | Loss: 0.2331\n",
      "\n",
      "🎨 Generating preview at step 1850...\n",
      "📦 Using current UNet for inference.\n",
      "🧠 Restored LoRA-trained text encoder for inference.\n",
      "✅ Trigger word 'pendugwen' is tokenized correctly: ['pendugwen']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f4a642f13bd4764a3e518c6ada9a12d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: /kaggle/working/gen_images/step_1850/preview_step_1850_1.png\n",
      "✅ Saved: /kaggle/working/gen_images/step_1850/preview_step_1850_2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [01:09<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: /kaggle/working/gen_images/step_1850/preview_step_1850_3.png\n",
      "✅ Saved: /kaggle/working/gen_images/step_1850/preview_step_1850_4.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:46<00:00,  1.25s/it]\n",
      " 78%|███████▊  | 29/37 [00:36<00:10,  1.26s/it]"
     ]
    }
   ],
   "source": [
    "from torch.amp import autocast\n",
    "from safetensors.torch import save_file\n",
    "from diffusers.models.attention_processor import LoRAAttnProcessor\n",
    "from PIL import Image\n",
    "\n",
    "# 🛠️ Optional: Force LoRA layers to float32 for stability\n",
    "for module in unet.modules():\n",
    "    if isinstance(module, LoRAAttnProcessor):\n",
    "        for param in module.parameters():\n",
    "            param.data = param.data.to(torch.float32)\n",
    "\n",
    "global_step = 0\n",
    "unet.train()\n",
    "\n",
    "for epoch in range(100):\n",
    "    for step, batch in enumerate(tqdm(dataloader)):\n",
    "        with accelerator.accumulate(unet):\n",
    "            # ✅ 1. Move batch to device\n",
    "            pixel_values = batch[\"pixel_values\"].to(accelerator.device, dtype=torch.float32)  # VAE prefers float32\n",
    "            input_ids = batch[\"input_ids\"].to(accelerator.device)\n",
    "            # print(tokenizer.decode(input_ids[0]))\n",
    "# \n",
    "            # ✅ 2. Encode with VAE (float32), then clamp and convert\n",
    "            with torch.no_grad():\n",
    "                latents = pipe.vae.encode(pixel_values).latent_dist.sample()\n",
    "                latents = latents.clamp(-10, 10)  # Avoid extreme latent values\n",
    "                latents = latents * 0.18215\n",
    "                latents = latents.to(accelerator.device, dtype=torch.float16)\n",
    "\n",
    "            # ✅ 3. Add scaled noise\n",
    "            noise = 0.9 * torch.randn_like(latents)  # reduce intensity for early stability\n",
    "            max_timestep = 300 if global_step < 100 else pipe.scheduler.config.num_train_timesteps\n",
    "            timesteps = torch.randint(0, max_timestep, (latents.shape[0],), device=latents.device).long()\n",
    "            noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "            for name, param in unet.named_parameters():\n",
    "                if torch.isnan(param).any() or torch.isinf(param).any():\n",
    "                    print(f\"[❌ NaN/Inf detected] in parameter: {name}\")\n",
    "                    break\n",
    "\n",
    "\n",
    "            # ✅ 4. Encode text\n",
    "            with torch.no_grad():\n",
    "                encoder_hidden_states = pipe.text_encoder(input_ids)[0]\n",
    "                encoder_hidden_states = encoder_hidden_states.to(accelerator.device, dtype=torch.float16)\n",
    "\n",
    "            # ✅ 5. UNet forward with autocast\n",
    "            with autocast(\"cuda\", dtype=torch.float32):\n",
    "                model_pred = unet(\n",
    "                    noisy_latents,\n",
    "                    timesteps,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                ).sample\n",
    "\n",
    "            # ✅ Debug logs\n",
    "            # print(f\"Latents mean/std: {latents.mean().item():.4f}/{latents.std().item():.4f}\")\n",
    "            # print(f\"model_pred mean/std: {model_pred.mean().item():.4f}/{model_pred.std().item():.4f}\")\n",
    "\n",
    "            # ✅ Check for NaNs\n",
    "            if torch.isnan(model_pred).any():\n",
    "                print(\"❌ NaN in model_pred!\")\n",
    "                print(\"Input stats:\", noisy_latents.mean(), noisy_latents.std())\n",
    "                print(\"Timesteps:\", timesteps)\n",
    "                print(\"Encoder stats:\", encoder_hidden_states.mean(), encoder_hidden_states.std())\n",
    "                continue\n",
    "\n",
    "            # ✅ 6. Compute loss\n",
    "            noise = noise.to(model_pred.dtype)\n",
    "            loss = torch.nn.functional.l1_loss(model_pred, noise)\n",
    "\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                print(f\"⚠️ Skipping invalid loss at step {global_step}\")\n",
    "                continue\n",
    "\n",
    "            # ✅ 7. Backward + optimizer\n",
    "            accelerator.backward(loss)\n",
    "            if accelerator.sync_gradients:\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                # ✅ Logging\n",
    "                if global_step % cfg.log_every_n_steps == 0:\n",
    "                    print(f\"Step {global_step} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "                # ✅ Save LoRA Weights\n",
    "                if global_step % cfg.save_every_n_steps == 0:\n",
    "                    save_path = os.path.join(cfg.output_dir, f\"step_{global_step}\")\n",
    "                    os.makedirs(save_path, exist_ok=True)\n",
    "                \n",
    "                    def extract_lora_weights(state_dict):\n",
    "                        return {k: v for k, v in state_dict.items() if \"lora\" in k.lower()}\n",
    "                \n",
    "                    # ✅ Extract only LoRA weights from UNet and text encoder\n",
    "                    unet_lora = extract_lora_weights(accelerator.unwrap_model(unet).state_dict())\n",
    "                    text_lora = extract_lora_weights(accelerator.unwrap_model(pipe.text_encoder).state_dict())\n",
    "                \n",
    "                    # ✅ Combine weights into a single dictionary\n",
    "                    combined_lora = {**unet_lora, **text_lora}\n",
    "                \n",
    "                    # ✅ Save using safetensors\n",
    "                    save_file(combined_lora, os.path.join(save_path, \"lora_only.safetensors\"))\n",
    "                \n",
    "                    print(f\"✅ Saved combined LoRA checkpoint at step {global_step} → {save_path}/lora_only.safetensors\")\n",
    "\n",
    "                    \n",
    "                # 🔍 Generate sample image\n",
    "                if global_step % cfg.generate_every_n_steps == 0:\n",
    "                    img_path = os.path.join('/kaggle/working/gen_images', f\"step_{global_step}\")\n",
    "                    generate_sample_image(global_step, img_path)\n",
    "\n",
    "        # ✅ Exit condition\n",
    "        if global_step >= cfg.max_train_steps:\n",
    "            break\n",
    "    if global_step >= cfg.max_train_steps:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7755416,
     "sourceId": 12318697,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7772174,
     "sourceId": 12336375,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7591671,
     "sourceId": 12350262,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7806797,
     "isSourceIdPinned": false,
     "sourceId": 12420729,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
