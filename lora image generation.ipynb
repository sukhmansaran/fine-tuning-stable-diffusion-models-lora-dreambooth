{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DreamBooth + LoRA Inference Pipeline for Stable Diffusion Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from diffusers import StableDiffusionPipeline, UNet2DConditionModel\n",
    "from diffusers.models.attention_processor import LoRAAttnProcessor\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HF TOKEN\n",
    "from huggingface_hub import login\n",
    "login(\"your_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading Realistic Vision V5\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "\n",
    "model_dir = \"your_dir_for_saving_downloaded_base_model\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "ckpt_path = hf_hub_download(\n",
    "    repo_id=\"SG161222/Realistic_Vision_V5.1_noVAE\",\n",
    "    filename=\"Realistic_Vision_V5.1.safetensors\",\n",
    "    local_dir=model_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is inference file necessary. The file has to be this exact file as it was used for stable diffusion 1.5 version you cannot use any other file here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Downloading v1-inference.yaml\n",
    "!wget -O v1-inference.yaml https://raw.githubusercontent.com/CompVis/stable-diffusion/main/configs/stable-diffusion/v1-inference.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to diffusers format\n",
    "from diffusers.pipelines.stable_diffusion.convert_from_ckpt import download_from_original_stable_diffusion_ckpt\n",
    "import os\n",
    "\n",
    "safetensors_path = \"downloaded_base_model_paths\"\n",
    "output_dir = \"your_dir_for_saving_converted_base_model\"\n",
    "\n",
    "converted_pipeline = download_from_original_stable_diffusion_ckpt(\n",
    "    safetensors_path,\n",
    "    \"/v1-inference.yaml\",  # Must match SD1.5 or SD2.x\n",
    "    from_safetensors=True,\n",
    "    extract_ema=True,\n",
    "    device=\"cuda\"  # or \"cpu\"\n",
    ")\n",
    "\n",
    "# saving\n",
    "converted_pipeline.save_pretrained(output_dir)\n",
    "print(f\"Model saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model page: https://huggingface.co/SG161222/Realistic_Vision_V5.1_noVAE\n",
    "\n",
    "‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/SG161222/Realistic_Vision_V5.1_noVAE)\n",
    "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Paths\n",
    "base_model_path = \"your_base_model_path\"  # your base model dir\n",
    "lora_path = \"your_lora_path\"  # folder containing your LORA weights\n",
    "output_dir = \"./outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the pipeline for loading the model and then using it for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.schedulers.scheduling_dpmsolver_multistep import DPMSolverMultistepScheduler\n",
    "\n",
    "# ‚úÖ Load pipeline\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    safety_checker=None,\n",
    "    requires_safety_checker=False,\n",
    ")\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code patches and loads trained weights for unet and text_encoder into the Stable Diffusion model used during the fine tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patching to_q, to_k, to_v and to_out\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "# ‚úÖ LoRA wrapper\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "\n",
    "        self.lora_down = nn.Linear(linear.in_features, rank, bias=False)\n",
    "        self.lora_up = nn.Linear(rank, linear.out_features, bias=False)\n",
    "\n",
    "        nn.init.zeros_(self.lora_up.weight)\n",
    "        nn.init.kaiming_uniform_(self.lora_down.weight, a=5**0.5)\n",
    "\n",
    "        self.lora_down.to(linear.weight.device, dtype=linear.weight.dtype)\n",
    "        self.lora_up.to(linear.weight.device, dtype=linear.weight.dtype)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora_up(self.lora_down(x)) * self.scaling\n",
    "\n",
    "# ‚úÖ Patch UNet with LoRA ‚Äî q, k, v, and to_out[0]\n",
    "def patch_unet_cross_attn_with_lora(unet, rank, alpha):\n",
    "    for module in unet.modules():\n",
    "        if hasattr(module, 'to_q') and hasattr(module, 'to_k') and hasattr(module, 'to_v'):\n",
    "            for attr in ['to_q', 'to_k', 'to_v']:\n",
    "                original = getattr(module, attr)\n",
    "                if isinstance(original, nn.Linear) and not isinstance(original, LoRALinear):\n",
    "                    lora_layer = LoRALinear(original, rank=rank, alpha=alpha).to(original.weight.device)\n",
    "                    setattr(module, attr, lora_layer)\n",
    "\n",
    "            # Handle to_out[0] if it's a linear layer\n",
    "            if hasattr(module, 'to_out') and isinstance(module.to_out, nn.ModuleList):\n",
    "                if isinstance(module.to_out[0], nn.Linear) and not isinstance(module.to_out[0], LoRALinear):\n",
    "                    original = module.to_out[0]\n",
    "                    lora_layer = LoRALinear(original, rank=rank, alpha=alpha).to(original.weight.device)\n",
    "                    module.to_out[0] = lora_layer\n",
    "\n",
    "# ‚úÖ Load LoRA weights\n",
    "def apply_lora_weights(unet, lora_path):\n",
    "    print(\"üîÑ Applying LoRA weights...\")\n",
    "    state_dict = load_file(lora_path, device=\"cuda\")\n",
    "    missing = []\n",
    "\n",
    "    for name, param in unet.named_parameters():\n",
    "        if \"lora\" in name:\n",
    "            if name in state_dict:\n",
    "                param.data.copy_(state_dict[name])\n",
    "            else:\n",
    "                missing.append(name)\n",
    "\n",
    "    print(\"‚úÖ LoRA weights loaded.\")\n",
    "    if missing:\n",
    "        print(\"‚ö†Ô∏è Missing LoRA keys:\", missing)\n",
    "\n",
    "# ‚úÖ Patch CLIP TextEncoder Attention (q_proj, k_proj, v_proj, out_proj)\n",
    "def patch_text_encoder_attention_with_lora(text_encoder, rank, alpha):\n",
    "    for module in text_encoder.modules():\n",
    "        if all(hasattr(module, attr) for attr in ['q_proj', 'k_proj', 'v_proj', 'out_proj']):\n",
    "            for attr in ['q_proj', 'k_proj', 'v_proj', 'out_proj']:\n",
    "                original = getattr(module, attr)\n",
    "                if isinstance(original, nn.Linear) and not isinstance(original, LoRALinear):\n",
    "                    lora_layer = LoRALinear(original, rank=rank, alpha=alpha).to(original.weight.device)\n",
    "                    setattr(module, attr, lora_layer)\n",
    "\n",
    "# ‚úÖ Load LoRA weights into Text Encoder\n",
    "def apply_lora_weights_to_text_encoder(text_encoder, lora_state_dict):\n",
    "    missing = []\n",
    "    for name, param in text_encoder.named_parameters():\n",
    "        if \"lora\" in name:\n",
    "            if name in lora_state_dict:\n",
    "                param.data.copy_(lora_state_dict[name])\n",
    "            else:\n",
    "                missing.append(name)\n",
    "    print(\"‚úÖ Text encoder LoRA weights loaded.\")\n",
    "    if missing:\n",
    "        print(\"‚ö†Ô∏è Missing text encoder LoRA keys:\", missing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function code patches and loads our unet LORA weights into our pipeline. You have to use the same rank and alpha used during the fine tuning process if they mismatch it will cause error and not patch the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Apply to loaded pipeline UNet\n",
    "patch_unet_cross_attn_with_lora(pipe.unet, rank=4, alpha=8)\n",
    "apply_lora_weights(pipe.unet, f\"{lora_path}/diffusion_pytorch_model.safetensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as unet this function code patches and loads our text_encoder LORA weights into our pipeline. You have to use the same rank and alpha used during the fine tuning process if they mismatch it will cause error and not patch the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Apply to loaded pipeline Text Encoder\n",
    "patch_text_encoder_attention_with_lora(pipe.text_encoder, rank=4, alpha=8)\n",
    "\n",
    "# Reuse same weights already loaded (avoid re-loading from disk)\n",
    "lora_state_dict = load_file(f\"{lora_path}/diffusion_pytorch_model.safetensors\", device=\"cuda\")\n",
    "apply_lora_weights_to_text_encoder(pipe.text_encoder, lora_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inference process make sure to use your trigger_word here. You can change your prompt, negative prompt, guidance scale, num_inference_steps, height and width. \n",
    "\n",
    "Guidance scale recommended is 5.5-7.5\n",
    "\n",
    "Num inference steps recommeded are 30-50\n",
    "\n",
    "You must use standard images sizes don't use custom sizes as you like as the model may give unnatural, messy results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Define inference settings\n",
    "prompt = f\"{trigger_word}, your prompt\"\n",
    "negative_prompt = (\n",
    "    \"blurry, low resolution, grainy, overexposed, underexposed, bad lighting, jpeg artifacts, glitch, \"\n",
    "    \"cropped, out of frame, watermark, duplicate, poorly drawn face, asymmetrical face, deformed features, \"\n",
    "    \"bad skin texture, doll-like face, bad eyes, mutated hands, extra fingers, unrealistic proportions, \"\n",
    "    \"cartoon, anime, illustration, painting, horror, morbid\"\n",
    ")\n",
    "guidance_scale = 6.5\n",
    "num_inference_steps = 30\n",
    "height = 768\n",
    "width = 768\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code generates and saves your images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Generate and save images\n",
    "image = pipe(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    height=height,\n",
    "    width=width,\n",
    "    num_inference_steps=num_inference_steps,\n",
    "    guidance_scale=guidance_scale\n",
    ").images[0]\n",
    "\n",
    "image.save(os.path.join(output_dir, \"output.png\"))\n",
    "# image.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "modelId": 384403,
     "modelInstanceId": 363537,
     "sourceId": 447791,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 392030,
     "modelInstanceId": 371132,
     "sourceId": 457785,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 392044,
     "modelInstanceId": 371146,
     "sourceId": 457825,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
