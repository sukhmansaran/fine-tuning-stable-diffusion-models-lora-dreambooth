{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Please read README.md file on my github for more information.\n\nhttps://github.com/sukhmansaran/fine-tuning-stable-diffusion-models-lora-dreambooth","metadata":{}},{"cell_type":"markdown","source":"**This notebook is part two for fine tuning model for a single character.**","metadata":{}},{"cell_type":"markdown","source":"# DreamBooth + LoRA Training Pipeline for Stable Diffusion Models","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport random\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\nfrom torchvision import transforms\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer\nfrom diffusers import StableDiffusionPipeline, UNet2DConditionModel\nfrom diffusers.models.attention_processor import LoRAAttnProcessor\nfrom accelerate import Accelerator","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# HF TOKEN\nfrom huggingface_hub import login\nlogin(\"your_token\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Downloading Realistic Vision V5\nfrom huggingface_hub import hf_hub_download\nimport os\n\nmodel_dir = \"your_dir_for_saving_downloaded_base_model\"\nos.makedirs(model_dir, exist_ok=True)\n\nckpt_path = hf_hub_download(\n    repo_id=\"SG161222/Realistic_Vision_V5.1_noVAE\",\n    filename=\"Realistic_Vision_V5.1.safetensors\",\n    local_dir=model_dir,\n)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This is inference file necessary. The file has to be this exact file as it was used for stable diffusion 1.5 version you cannot use any other file here.","metadata":{}},{"cell_type":"code","source":"# @title Downloading v1-inference.yaml\n!wget -O v1-inference.yaml https://raw.githubusercontent.com/CompVis/stable-diffusion/main/configs/stable-diffusion/v1-inference.yaml","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert to diffusers format\nfrom diffusers.pipelines.stable_diffusion.convert_from_ckpt import download_from_original_stable_diffusion_ckpt\n\nsafetensors_path = \"downloaded_base_model_path\"\noutput_dir = \"your_dir_for_saving_converted_base_model\"\n\nconverted_pipeline = download_from_original_stable_diffusion_ckpt(\n    safetensors_path,\n    \"/v1-inference.yaml\",  # Must match SD1.5 or SD2.x\n    from_safetensors=True,\n    extract_ema=True,\n    device=\"cuda\"  # or \"cpu\"\n)\n\n# saving\nconverted_pipeline.save_pretrained(output_dir)\nprint(f\"Model saved to {output_dir}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Loading the phase 1 config and setting up config for phase 2. Phase 1 config file is necessary as we need to extract rank and alpha from that file.\n\nThe current config settings are what I have tested and experimented with and have worked for me you can experiment with changing the config settings.","metadata":{}},{"cell_type":"code","source":"# import json\nimport json\n\nclass Config:\n    def __init__(self, json_path=None):\n        if json_path:\n            self.load_from_json(json_path)\n        else:\n            self.set_defaults()\n\n    def load_from_json(self, path):\n        with open(path, 'r') as f:\n            data = json.load(f)\n        for key, value in data.items():\n            setattr(self, key, value)\n\n    def set_defaults(self):\n        # ‚úÖ Phase 2 Defaults\n        self.model_path = \"your_downloaded_model_path\"\n        self.dataset_dir = \"your_dataset_path\"\n        self.output_dir = \"your_output_dir\"\n\n        self.trigger_word = \"sks\"\n        self.resolution = 512\n        self.batch_size = 1\n        self.gradient_accumulation = 4\n        self.learning_rate = 4e-5\n        self.max_train_steps = 3240\n        self.mixed_precision = \"fp16\"\n        self.train_text_encoder = True\n\n        self.lr_scheduler = \"cosine\"\n        self.lr_warmup_steps = 185\n        self.lora_r = 4\n        self.lora_alpha = 8\n        self.lora_dropout = 0.1\n\n        self.save_every_n_steps = 810\n        self.log_every_n_steps = 405\n        self.generate_every_n_steps = 405\n        self.seed = 151101\n\n        # Optional Phase 1 Reference\n        self.phase1_config_path = \"your_path_to_phase1_config_file\"\n        self.phase1_lora_path = \"your_path_to_phase1_lora_weights\"\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cfg = Config()\nphase1_cfg = Config(json_path=cfg.phase1_config_path)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, json, inspect\n\ndef save_config(cfg, path=None):\n    if path is None:\n        path = os.path.join(cfg.output_dir, \"lora_config.json\")\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n\n    # grab only data attributes declared on the *class*\n    config_dict = {\n        k: v for k, v in cfg.__class__.__dict__.items()\n        if not k.startswith(\"__\") and not inspect.isfunction(v) and not inspect.ismethod(v)\n    }\n\n    with open(path, \"w\") as f:\n        json.dump(config_dict, f, indent=4)\n    print(f\"‚úÖ  Config saved to {path}\")\n\nsave_config(cfg)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reproducibility\ntorch.manual_seed(cfg.seed)\ntorch.cuda.manual_seed(cfg.seed)\nrandom.seed(cfg.seed)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dataset Loader\nclass ImageCaptionDataset(Dataset):\n    def __init__(self, image_dir, tokenizer, size):\n        self.image_paths = []\n        self.caption_paths = []\n        self.tokenizer = tokenizer\n\n        for fname in sorted(os.listdir(image_dir)):\n            if fname.endswith(\".png\") or fname.endswith(\".jpg\"):\n                img_path = os.path.join(image_dir, fname)\n                txt_path = os.path.splitext(img_path)[0] + \".txt\"\n                if os.path.exists(txt_path):\n                    self.image_paths.append(img_path)\n                    self.caption_paths.append(txt_path)\n\n        self.image_transforms = transforms.Compose([\n            transforms.Resize((size, size), interpolation=transforms.InterpolationMode.BILINEAR),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n\n        ])\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n        if image.getbbox() is None:\n            raise ValueError(f\"Empty image found: {self.image_paths[idx]}\")\n        else:\n            image = self.image_transforms(image)\n            with open(self.caption_paths[idx], \"r\") as f:\n                caption = f.read().strip()\n\n            inputs = self.tokenizer(caption, truncation=True, padding=\"max_length\", max_length=77, return_tensors=\"pt\")\n            \n        return {\"pixel_values\": image, \"input_ids\": inputs.input_ids.squeeze(0)}","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This is our class for adding two loras. It is necessary to patch 2 LORAs for phase 1 and phase 2. It will make sure that phase 1 loaded weights are frozen and accidently not trainable. If we make them trainable we might mess up the phase 1 weights and features learned during fine tuning. You can load the phase 1 weights and try to update those only and not adding new weights for training the fully body with face. I did because in phase 1 I made sure this weights learned the face features correctly and nicely, and in phase 2 it learned the rest of the body as in lower resolution like 512x512 the face features might not be learned correctly and can become messy. I use phase 1 weights for face and phase 2 weights for rest of the body. I am still experimenting it during inference and testing out what works best.  ","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass LoRALinearDualPhase(nn.Module):\n    def __init__(self, base_linear: nn.Linear, rank1, alpha1, rank2, alpha2):\n        super().__init__()\n        self.base = base_linear\n\n        # Phase 1 (frozen)\n        self.lora_down = nn.Linear(base_linear.in_features, rank1, bias=False)\n        self.lora_up   = nn.Linear(rank1, base_linear.out_features, bias=False)\n        self.scale     = alpha1 / rank1\n\n        # Phase 2 (trainable)\n        self.lora2_down = nn.Linear(base_linear.in_features, rank2, bias=False)\n        self.lora2_up   = nn.Linear(rank2, base_linear.out_features, bias=False)\n        self.scale2     = alpha2 / rank2\n\n        # Init: Phase 1 weights will be loaded, Phase 2 starts fresh\n        nn.init.kaiming_uniform_(self.lora2_down.weight, a=5**0.5)\n        nn.init.zeros_(self.lora2_up.weight)\n\n        # Freeze Phase 1\n        for p in self.lora_down.parameters(): p.requires_grad = False\n        for p in self.lora_up.parameters():   p.requires_grad = False\n\n    def forward(self, x):\n        base_out = self.base(x)\n        lora1_out = self.lora_up(self.lora_down(x)) * self.scale\n        lora2_out = self.lora2_up(self.lora2_down(x)) * self.scale2\n        return base_out + lora1_out + lora2_out\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This is patcher for patching 2 loras into the unet. Phase 1 weights will be patched with phase 1 rank and alpha and phase 2 lora will be added using phase 2 rank and alpha. The rank and alpha of phase 1 must be the same as used during the updating or training of those weights as not having correct weights will lead to patching error. ","metadata":{}},{"cell_type":"code","source":"def patch_unet_dual_lora(unet, rank1, alpha1, rank2, alpha2):\n    trainable_params = []\n\n    for module in unet.modules():\n        for attr in ['to_q', 'to_k', 'to_v', 'to_out']:\n            if hasattr(module, attr):\n                orig = getattr(module, attr)\n\n                if isinstance(orig, nn.Linear):\n                    dual_lora = LoRALinearDualPhase(orig, rank1, alpha1, rank2, alpha2)\n                    setattr(module, attr, dual_lora)\n                    trainable_params += list(dual_lora.lora2_down.parameters())\n                    trainable_params += list(dual_lora.lora2_up.parameters())\n                    print(f\"‚úÖ Patched {module.__class__.__name__}.{attr} with dual-phase LoRA\")\n\n                elif isinstance(orig, nn.ModuleList):\n                    for i, sub in enumerate(orig):\n                        if isinstance(sub, nn.Linear):\n                            dual_lora = LoRALinearDualPhase(sub, rank1, alpha1, rank2, alpha2)\n                            orig[i] = dual_lora\n                            trainable_params += list(dual_lora.lora2_down.parameters())\n                            trainable_params += list(dual_lora.lora2_up.parameters())\n                            print(f\"‚úÖ Patched {module.__class__.__name__}.{attr}[{i}] with dual-phase LoRA\")\n\n    print(f\"‚úÖ Total trainable UNet Phase 2 params: {sum(p.numel() for p in trainable_params):,}\")\n    return trainable_params\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The same applies in the text encoder patching up. This is patcher for patching 2 loras into the text encoder. Phase 1 weights will be patched with phase 1 rank and alpha and phase 2 lora will be added using phase 2 rank and alpha. The rank and alpha of phase 1 must be the same as used during the updating or training of those weights as not having correct weights will lead to patching error. ","metadata":{}},{"cell_type":"code","source":"def patch_text_encoder_dual_lora(text_encoder, rank1, alpha1, rank2, alpha2):\n    trainable_params = []\n    target_names = {\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"}\n\n    for module in text_encoder.modules():\n        for name in target_names:\n            if hasattr(module, name):\n                orig = getattr(module, name)\n                if isinstance(orig, nn.Linear):\n                    dual_lora = LoRALinearDualPhase(orig, rank1, alpha1, rank2, alpha2)\n                    setattr(module, name, dual_lora)\n                    trainable_params += list(dual_lora.lora2_down.parameters())\n                    trainable_params += list(dual_lora.lora2_up.parameters())\n                    print(f\"üîß Patched {module.__class__.__name__}.{name} with dual-phase LoRA\")\n\n    print(f\"‚úÖ Total trainable Text Encoder Phase 2 params: {sum(p.numel() for p in trainable_params):,}\")\n    return trainable_params\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This function loads the phase 1 weights into our patched LORA for phase 1. It loads unet and text_encoder weights into the LORA.","metadata":{}},{"cell_type":"code","source":"from safetensors.torch import load_file\n\ndef load_phase1_lora_weights(unet, text_encoder, weights_path):\n    state_dict = load_file(weights_path)\n\n    n_loaded = 0\n    for name, module in list(unet.named_modules()) + list(text_encoder.named_modules()):\n        if isinstance(module, LoRALinearDualPhase):\n            for part, param in [('lora_down', module.lora_down), ('lora_up', module.lora_up)]:\n                key = f\"{name}.{part}.weight\"\n                if key in state_dict:\n                    with torch.no_grad():\n                        param.weight.copy_(state_dict[key])\n                        n_loaded += 1\n                else:\n                    print(f\"‚ö†Ô∏è Missing key in weights: {key}\")\n    print(f\"‚úÖ Loaded {n_loaded} Phase 1 LoRA weights\")\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def setup_phase2(unet, text_encoder, rank_phase1, alpha_phase1, rank_phase2, alpha_phase2,\n                 train_text_encoder, load_phase1_path):\n\n    # Patch UNet\n    lora_params = patch_unet_dual_lora(unet, rank_phase1, alpha_phase1, rank_phase2, alpha_phase2)\n\n    # Patch Text Encoder\n    if train_text_encoder:\n        lora_params += patch_text_encoder_dual_lora(text_encoder, rank_phase1, alpha_phase1, rank_phase2, alpha_phase2)\n\n    # Load Phase 1 LoRA weights\n    load_phase1_lora_weights(unet, text_encoder, load_phase1_path)\n\n    return lora_params\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now this is where we would create our pipeline, load our model into pipeline, load our tokenizer, load our trigger word into the tokenizer, load our unet and patch it, load our text_encoder and patch it. But the important thing is that we need to freeze our params of unet and text_encoder before patching so we accidently don't train them or update them and we only train or update our newly patched layers. We will also freeze phase 1 weights here for unet and text encoder so they don't get updated or trained accidently. We are using AdamW optimer here originally used with base our base model. We would also load our dataset into the pipeline and prepare everything for training.","metadata":{}},{"cell_type":"code","source":"import os, torch\nfrom accelerate import Accelerator\nfrom diffusers import StableDiffusionPipeline, UNet2DConditionModel\nfrom transformers import AutoTokenizer, get_scheduler\nfrom torch.utils.data import DataLoader\n\n# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n# 1. Accelerator & device setup\n# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\naccelerator = Accelerator(split_batches=True)\ndevice = accelerator.device\n\n# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n# 2. Load Stable Diffusion base pipeline\n# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\npipe = StableDiffusionPipeline.from_pretrained(\n    cfg.model_path,\n    torch_dtype=torch.float16\n).to(device)\n\n# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n# 3. Add trigger word (tokenize before dataset creation!)\n# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\ntokenizer = AutoTokenizer.from_pretrained(cfg.model_path, subfolder=\"tokenizer\")\ntrigger_token = cfg.trigger_word\n\nif len(tokenizer.tokenize(trigger_token)) > 1:\n    tokenizer.add_tokens([trigger_token])\n    pipe.text_encoder.resize_token_embeddings(len(tokenizer))\n    with torch.no_grad():\n        emb = pipe.text_encoder.get_input_embeddings()\n        new_id = tokenizer.convert_tokens_to_ids(trigger_token)\n        base_id = tokenizer.convert_tokens_to_ids(\"person\")\n        emb.weight[new_id] = emb.weight[base_id].clone()\n    print(f\"‚úÖ Added custom token '{trigger_token}' (id {new_id})\")\nelse:\n    print(f\"‚úÖ '{trigger_token}' is already a single token\")\n\npipe.tokenizer = tokenizer\n\n# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n# 4. Load UNet and freeze base weights\n# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nunet = UNet2DConditionModel.from_pretrained(\n    cfg.model_path,\n    subfolder=\"unet\",\n    torch_dtype=torch.float16\n).to(device)\n\n# Freeze base model\nfor p in unet.parameters(): p.requires_grad = False\nfor p in pipe.text_encoder.parameters(): p.requires_grad = False\n\n# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n# 5. Patch UNet and text encoder with dual-phase LoRA\n# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nlora_params = setup_phase2(\n    unet               = unet,\n    text_encoder       = pipe.text_encoder,\n    rank_phase1        = phase1_cfg.lora_r,\n    alpha_phase1       = phase1_cfg.lora_alpha,\n    rank_phase2        = cfg.lora_r,\n    alpha_phase2       = cfg.lora_alpha,\n    train_text_encoder = cfg.train_text_encoder,\n    load_phase1_path   = cfg.phase1_lora_path,\n)\n\nif not lora_params:\n    raise RuntimeError(\"‚ùå No LoRA parameters collected!\")\n\nprint(f\"üîç LoRA trainable parameters: {sum(p.numel() for p in lora_params):,}\")\n\n# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n# 6. Optimizer and LR Scheduler\n# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\noptimizer = torch.optim.AdamW(lora_params, lr=cfg.learning_rate, fused=False)\n\nlr_scheduler = get_scheduler(\n    cfg.lr_scheduler,\n    optimizer=optimizer,\n    num_warmup_steps=cfg.lr_warmup_steps,\n    num_training_steps=cfg.max_train_steps,\n)\n\n# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n# 7. Dataset and Dataloader\n# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\ndataset = ImageCaptionDataset(cfg.dataset_dir, tokenizer, cfg.resolution)\ndataloader = DataLoader(dataset, batch_size=cfg.batch_size, shuffle=True)\n\n# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n# 8. Final Prep\n# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\npipe.text_encoder.to(device, dtype=torch.float16)\npipe.text_encoder.train()\nunet.train()\n\npipe.text_encoder, unet, optimizer, dataloader = accelerator.prepare(pipe.text_encoder, unet, optimizer, dataloader)\n\n\nprint(\"üöÄ Setup complete ‚Äì ready to train Phase 2 LoRA adapters.\")\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Sanity check for how many parameters are trainable out of total parameters.","metadata":{}},{"cell_type":"code","source":"# Count total and trainable params across both modules\ntotal_params = sum(p.numel() for p in unet.parameters()) + sum(p.numel() for p in pipe.text_encoder.parameters())\ntrainable_params = sum(p.numel() for p in unet.parameters() if p.requires_grad) + \\\n                   sum(p.numel() for p in pipe.text_encoder.parameters() if p.requires_grad)\n\ntrainable_percent = (trainable_params / total_params) * 100\nprint(f\"‚úÖ Trainable parameters: {trainable_params:,} / {total_params:,} ({trainable_percent:.4f}%)\")\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Loading the vae used in the base model. The vae is what encodes and decodes our latent spaces into images.","metadata":{}},{"cell_type":"code","source":"from diffusers import AutoencoderKL\n\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\", torch_dtype=torch.float32)\npipe.vae = vae\n\npipe.vae.to(accelerator.device, dtype=torch.float32)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(accelerator.device)  # Should print: cuda\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Sanity check that if our paramters contain NAN values before our training begins.","metadata":{}},{"cell_type":"code","source":"for name, param in unet.named_parameters():\n    if torch.isnan(param).any():\n        print(f\"NaN detected in UNet parameter: {name}\")\n        break","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we would create functions for checking if our trigger_word is in the tokenizer, a function to generate sample images during the model training for checks if our model is learning or not, whether it is overfitting or underfitting. Whether if we need to stop our training, whether we need to change our configurations settings.","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport torch\nfrom diffusers import DDIMScheduler\nfrom torch.cuda.amp import autocast  # More up-to-date than torch.autocast\nfrom torchvision.utils import save_image  # Optional if using tensor images\n\n# ‚úÖ Optional: Reproducibility\ndef seed_everything(seed=151101):\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n# ‚úÖ Optional: Disable NSFW filter (for local testing)\ndef disable_safety(pipe):\n    if hasattr(pipe, \"safety_checker\"):\n        pipe.safety_checker = lambda images, **kwargs: (images, [False] * len(images))\n        print(\"üõë NSFW safety checker disabled.\")\n    else:\n        print(\"‚ö†Ô∏è No safety checker found in pipeline.\")\n\n# ‚úÖ Fix tokenizer trigger word if needed\ndef ensure_trigger_token(pipe, trigger_word):\n    tokens = pipe.tokenizer.tokenize(trigger_word)\n    if len(tokens) > 1:\n        print(f\"‚ö†Ô∏è Trigger word '{trigger_word}' splits into: {tokens}. Fixing...\")\n        pipe.tokenizer.add_tokens([trigger_word])\n        pipe.text_encoder.resize_token_embeddings(len(pipe.tokenizer))\n\n        with torch.no_grad():\n            embeddings = pipe.text_encoder.get_input_embeddings()\n            new_id = pipe.tokenizer.convert_tokens_to_ids(trigger_word)\n            base_id = pipe.tokenizer.convert_tokens_to_ids(\"person\")\n            embeddings.weight[new_id] = embeddings.weight[base_id].clone()\n\n        print(f\"‚úÖ Re-initialized token embedding for '{trigger_word}'\")\n    else:\n        print(f\"‚úÖ Trigger word '{trigger_word}' is already tokenized correctly: {tokens}\")\n\n# ‚úÖ Generate and save preview images\ndef generate_sample_image(step, save_path, prompt=None, negative_prompt=None, seed=151101):\n    print(f\"\\nüé® Generating preview at step {step}...\")\n\n    seed_everything(seed)\n    generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n\n    # Restore UNet (EMA or current)\n    try:\n        if 'ema_unet' in globals() and ema_unet is not None:\n            pipe.unet = ema_unet\n            print(\"üì¶ Using EMA UNet.\")\n        else:\n            pipe.unet = accelerator.unwrap_model(unet)\n            print(\"üì¶ Using current LoRA-patched UNet.\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Error restoring UNet: {e}\")\n\n    # Restore text encoder if needed\n    if cfg.train_text_encoder:\n        try:\n            pipe.text_encoder = accelerator.unwrap_model(pipe.text_encoder)\n            print(\"üß† Restored LoRA-patched text encoder.\")\n        except Exception as e:\n            print(f\"‚ö†Ô∏è Failed to unwrap text encoder: {e}\")\n\n    pipe.unet.eval()\n    pipe.text_encoder.eval()\n\n    disable_safety(pipe)\n    ensure_trigger_token(pipe, cfg.trigger_word)\n\n    # Replace scheduler with DDIM for faster inference\n    pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n\n    # Default prompt if not given\n    if not prompt:\n        prompt = f\"{cfg.trigger_word} your prompt\"\n    if not negative_prompt:\n        negative_prompt = (\n            \"blurry, low resolution, jpeg artifacts, cropped, watermark, distorted face, extra limbs, poorly drawn, cartoon\"\n        )\n\n    # Generate\n    with torch.no_grad(), autocast(\"cuda\"):\n        result = pipe(\n            prompt=[prompt] * 4,\n            negative_prompt=[negative_prompt] * 4,\n            num_inference_steps=30,\n            guidance_scale=6.0,\n            height=cfg.resolution,\n            width=cfg.resolution,\n            generator=generator,\n        )\n\n    # Save\n    os.makedirs(save_path, exist_ok=True)\n    for i, image in enumerate(result.images):\n        save_path_i = os.path.join(save_path, f\"preview_step_{step}_{i+1}.png\")\n        image.save(save_path_i)\n        print(f\"‚úÖ Saved: {save_path_i}\")\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Our main training loop for training our model. ","metadata":{}},{"cell_type":"code","source":"from torch.amp import autocast  # ‚ö†Ô∏è Correct to torch.cuda.amp.autocast\nfrom safetensors.torch import save_file\nfrom diffusers.models.attention_processor import LoRAAttnProcessor\nfrom PIL import Image\nimport torch, os\nfrom tqdm import tqdm\n\n# ‚úÖ LoRA layers to float32 for stability\n# for module in unet.modules():\n#     if isinstance(module, LoRAAttnProcessor):\n#         for param in module.parameters():\n#             param.data = param.data.to(torch.float32)\n\nglobal_step = 0\nunet.train()\n\nfor epoch in range(100):\n    for step, batch in enumerate(tqdm(dataloader)):\n        with accelerator.accumulate(unet):\n            pixel_values = batch[\"pixel_values\"].to(accelerator.device, dtype=torch.float32)\n            input_ids = batch[\"input_ids\"].to(accelerator.device)\n\n            with torch.no_grad(), torch.autocast(\"cuda\", dtype=torch.float16):\n                pixel_values = pixel_values.to(dtype=torch.float16)  # ‚úÖ convert input\n                latents = pipe.vae.encode(pixel_values).latent_dist.sample()\n                latents = latents.clamp(-10, 10)\n                latents = latents * 0.18215\n                latents = latents.to(accelerator.device, dtype=torch.float16)\n\n            noise = 0.9 * torch.randn_like(latents)\n            max_timestep = 300 if global_step < 100 else pipe.scheduler.config.num_train_timesteps\n            timesteps = torch.randint(0, max_timestep, (latents.shape[0],), device=latents.device).long()\n            noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)\n\n            # ‚úÖ NaN check for LoRA params\n            for name, param in unet.named_parameters():\n                if torch.isnan(param).any() or torch.isinf(param).any():\n                    print(f\"[‚ùå NaN/Inf detected] in: {name}\")\n                    break\n\n            with torch.no_grad():\n                encoder_hidden_states = pipe.text_encoder(input_ids)[0]\n                encoder_hidden_states = encoder_hidden_states.to(accelerator.device, dtype=torch.float16)\n\n            # ‚úÖ Use correct autocast context\n            with autocast(\"cuda\", dtype=torch.float32):\n                model_pred = unet(\n                    noisy_latents,\n                    timesteps,\n                    encoder_hidden_states=encoder_hidden_states,\n                ).sample\n\n            if torch.isnan(model_pred).any():\n                print(\"‚ùå NaN in model_pred!\")\n                continue\n\n            noise = noise.to(model_pred.dtype)\n            loss = torch.nn.functional.l1_loss(model_pred, noise)\n\n            if torch.isnan(loss) or torch.isinf(loss):\n                print(f\"‚ö†Ô∏è Skipping invalid loss at step {global_step}\")\n                continue\n\n            accelerator.backward(loss)\n            \n            if accelerator.sync_gradients:\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n\n                if global_step % cfg.log_every_n_steps == 0:\n                    print(f\"Step {global_step} | Loss: {loss.item():.4f}\")\n\n                # ‚úÖ Save Combined LoRA Weights\n                if global_step % cfg.save_every_n_steps == 0:\n                    save_path = os.path.join(cfg.output_dir, f\"step_{global_step}\")\n                    os.makedirs(save_path, exist_ok=True)\n\n                    def extract_lora_weights(state_dict):\n                        return {k: v for k, v in state_dict.items() if \"lora\" in k.lower()}\n\n                    unet_lora = extract_lora_weights(accelerator.unwrap_model(unet).state_dict())\n                    text_lora = extract_lora_weights(accelerator.unwrap_model(pipe.text_encoder).state_dict())\n                    combined_lora = {**unet_lora, **text_lora}\n\n                    save_file(combined_lora, os.path.join(save_path, \"lora_only.safetensors\"))\n                    print(f\"‚úÖ Saved combined LoRA weights ‚Üí {save_path}/lora_only.safetensors\")\n\n                # üîç Preview\n                if global_step % cfg.generate_every_n_steps == 0:\n                    preview_dir = os.path.join(\"/kaggle/working/gen_images\", f\"step_{global_step}\")\n                    generate_sample_image(global_step, preview_dir)\n\n                \n                global_step += 1\n\n        if global_step >= cfg.max_train_steps:\n            break\n    if global_step >= cfg.max_train_steps:\n        break\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The code for saving both phase 1 and phase 2 weights at the end of the training into a single file. It contains unet, text_encoder weights into a single lora.","metadata":{}},{"cell_type":"code","source":"# # Final save\n# # Save both UNet and text encoder\n# unet_state_dict = accelerator.unwrap_model(unet).state_dict()\n# text_encoder_state_dict = accelerator.unwrap_model(pipe.text_encoder).state_dict()\n\n# # Combine and save as safetensors\n# save_file({**unet_state_dict, **text_encoder_state_dict}, \"full_model_lora.safetensors\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}